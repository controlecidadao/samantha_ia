{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4b3513-0694-46ce-ad99-f73fce3517b5",
   "metadata": {},
   "source": [
    "# llama.cpp Playground"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf98acbf-7c28-431e-8fea-1dee6e272064",
   "metadata": {},
   "source": [
    "### Download model just for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab836024-569d-4db4-9322-fa73c4334a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from pathlib import Path\n",
    "from urllib.parse import urlparse\n",
    "import shutil\n",
    "import urllib3\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "url = \"https://huggingface.co/Qwen/Qwen2-0.5B-Instruct-GGUF/resolve/main/qwen2-0_5b-instruct-q8_0.gguf?download=true\"\n",
    "\n",
    "def download_model(url):\n",
    "    try:\n",
    "        # Extrair o caminho completo da pasta \"Downloads\"\n",
    "        downloads_path = Path(os.path.expanduser(\"~\")) / \"Downloads\"\n",
    "        print('Downloads path:', downloads_path)\n",
    "    \n",
    "        # Tentar remover o arquivo existente, se houver\n",
    "        try:\n",
    "            os.remove(downloads_path / \"MODEL_FOR_TESTING.gguf\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        # Obter o nome original do arquivo da URL\n",
    "        original_filename = os.path.basename(urlparse(url).path) # HERE THE MODEL NAME\n",
    "        \n",
    "        # Caminho tempor√°rio para o arquivo baixado\n",
    "        temp_file_path = downloads_path / original_filename\n",
    "        \n",
    "        # Baixar e salvar o arquivo diretamente no disco\n",
    "        with requests.get(url, stream=True, verify=False) as response:\n",
    "            response.raise_for_status()\n",
    "            with open(temp_file_path, 'wb') as file:\n",
    "                shutil.copyfileobj(response.raw, file) # Download takes place here\n",
    "        \n",
    "        # Renomear o arquivo \".gguf\"\n",
    "        new_file_path = downloads_path / \"MODEL_FOR_TESTING.gguf\"\n",
    "        os.rename(temp_file_path, new_file_path)\n",
    "\n",
    "        print(f\"Modelo salvo em: {new_file_path}\")\n",
    "        \n",
    "        # Retornar o caminho completo para o arquivo\n",
    "        return str(new_file_path)\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Erro ao baixar o arquivo: {e}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro inesperado: {e}\")\n",
    "        return None\n",
    "\n",
    "model = str(download_model(url))\n",
    "model = model.replace('\\\\', '\\\\\\\\')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd962f75-d9f1-410f-887c-69809599c1e1",
   "metadata": {},
   "source": [
    "### Local models path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9631632c-cd34-47fe-a283-08e9182d2704",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = r\"local_model_path\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f32449-3779-4220-81d1-af27a2192866",
   "metadata": {},
   "source": [
    "### llama.cpp version 0.2.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573eb86-6a44-4de3-93a1-1dce4c010afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "for i in dir(Llama):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e5c397-a8a6-4bec-b941-b57f21247e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del llm\n",
    "except:\n",
    "    pass\n",
    "\n",
    "llm = Llama(\n",
    "    model_path = f'{model}',\n",
    "    #*,\n",
    "\n",
    "    # Model Params\n",
    "    n_gpu_layers = 0,\n",
    "    split_mode = 1,\n",
    "    main_gpu = 0,\n",
    "    tensor_split = None,\n",
    "    rpc_servers = None,\n",
    "    vocab_only = False,\n",
    "    use_mmap = True,\n",
    "    use_mlock = False,\n",
    "    kv_overrides = None,\n",
    "    \n",
    "    # Context Params\n",
    "    seed = 4294967295,\n",
    "    n_ctx = 512,\n",
    "    n_batch = 512,\n",
    "    n_threads = None,\n",
    "    n_threads_batch = None,\n",
    "    rope_scaling_type = -1,\n",
    "    pooling_type = -1,\n",
    "    rope_freq_base = 0.0,\n",
    "    rope_freq_scale = 0.0,\n",
    "    yarn_ext_factor = -1.0,\n",
    "    yarn_attn_factor = 1.0,\n",
    "    yarn_beta_fast = 32.0,\n",
    "    yarn_beta_slow = 1.0,\n",
    "    yarn_orig_ctx = 0,\n",
    "    logits_all = False,\n",
    "    embedding = False,\n",
    "    offload_kqv = True,\n",
    "    flash_attn = False,\n",
    "    \n",
    "    # Sampling Params\n",
    "    last_n_tokens_size = 64,\n",
    "    \n",
    "    # LoRA Params\n",
    "    lora_base= None,\n",
    "    lora_scale = 1.0,\n",
    "    lora_path = None,\n",
    "    \n",
    "    # Backend Params\n",
    "    numa = False,\n",
    "    \n",
    "    # Chat Format Params\n",
    "    chat_format = None,\n",
    "    chat_handler = None,\n",
    "    \n",
    "    # Speculative Decoding\n",
    "    draft_model = None,\n",
    "    \n",
    "    # Tokenizer Override\n",
    "    tokenizer = None,\n",
    "    \n",
    "    # KV cache quantization\n",
    "    type_k = None,\n",
    "    type_v = None,\n",
    "    \n",
    "    # Misc\n",
    "    spm_infill = False,\n",
    "    verbose = True,\n",
    "    \n",
    "    # Extra Params\n",
    "    #**kwargs,  # type: ignore\n",
    ")\n",
    "\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab046bf-c62a-463d-b774-63395f011ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dir(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f13a66-ebe7-4121-9b2a-371027800866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prompt = \"Hello!\"\n",
    "\n",
    "messages = [\n",
    "            #{'role': 'system', 'content': ''},\n",
    "            {'role': 'user', 'content': ''},\n",
    "            {'role': 'assistant', 'content': ''},\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            #{'role': 'assistant', 'content': 'Follows answer in Chinese:'},\n",
    "           ]\n",
    "\n",
    "for n, i in enumerate(llm.create_chat_completion(\n",
    "    messages = messages,\n",
    "    functions = None,\n",
    "    function_call = None,\n",
    "    tools = None,\n",
    "    tool_choice = None,\n",
    "    temperature = 0.2,\n",
    "    top_p = 0.95,\n",
    "    top_k = 40,\n",
    "    min_p = 0.05,\n",
    "    typical_p = 1.0,\n",
    "    stream = True, #False, # <<<<<<<<<<<<<<<< Changed\n",
    "    stop = [],\n",
    "    seed = None,\n",
    "    response_format = None,\n",
    "    max_tokens = None,\n",
    "    presence_penalty = 0.0,\n",
    "    frequency_penalty = 0.0,\n",
    "    repeat_penalty = 1.1,\n",
    "    tfs_z = 1.0,\n",
    "    mirostat_mode = 0,\n",
    "    mirostat_tau = 5.0,\n",
    "    mirostat_eta = 0.1,\n",
    "    model = None,\n",
    "    logits_processor = None,\n",
    "    grammar = None,\n",
    "    logit_bias = None,\n",
    "    logprobs = None,\n",
    "    top_logprobs = None,\n",
    "    )):\n",
    "\n",
    "    try:\n",
    "        text = i['choices'][0]['delta']['content']\n",
    "        print(text, end='', flush=True)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7fa820-946a-44d7-9a18-0881de8a3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.detokenize(llm._input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9fef60-df63-4f66-a3cb-683e97d036f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm._input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b68d97-79d8-4038-be10-659a09b54f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(llm._input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fa198d-58d4-4808-9ec0-c5736934ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f8b71f-96ba-4017-92fb-3c915ac123a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
