{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d4b3513-0694-46ce-ad99-f73fce3517b5",
   "metadata": {},
   "source": [
    "# llama.cpp tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728fbd81-072a-4756-9bda-e4ce2d6cce81",
   "metadata": {},
   "outputs": [],
   "source": [
    " # NOT WORKING (ALL QWEN MODELS)\n",
    "# model = r\"C:\\Users\\t203771\\Downloads\\qwen2-0_5b-instruct-q4_k_m.gguf\"\n",
    "\n",
    "# WORKING\n",
    "model = r\"C:\\Users\\t203771\\Downloads\\Mistral-7B-Instruct-v0.3.Q5_K_M.gguf\"\n",
    "model = r\"C:\\Users\\t203771\\Downloads\\bling-phi-3.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e573eb86-6a44-4de3-93a1-1dce4c010afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6e5c397-a8a6-4bec-b941-b57f21247e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 23 key-value pairs and 195 tensors from C:\\Users\\t203771\\Downloads\\bling-phi-3.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.name str              = Phi3\n",
      "llama_model_loader: - kv   2:                        phi3.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv   4:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv   5:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   7:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   8:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv   9:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {{ bos_token }}{% for message in mess...\n",
      "llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:   81 tensors\n",
      "llama_model_loader: - type q5_K:   32 tensors\n",
      "llama_model_loader: - type q6_K:   17 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 323/32064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 2.23 GiB (5.01 BPW) \n",
      "llm_load_print_meta: general.name     = Phi3\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors:        CPU buffer size =  2281.66 MiB\n",
      "...........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   300.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi3', 'general.architecture': 'phi3', 'phi3.context_length': '4096', 'phi3.attention.head_count_kv': '32', 'phi3.embedding_length': '3072', 'phi3.block_count': '32', 'phi3.attention.head_count': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.rope.dimension_count': '96', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.padding_token_id': '32000', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% for message in messages %}{{'<|' + message['role'] + '|>' + '\\n' + message['content'] + '<|end|>\\n' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\"}\n",
      "Using gguf chat template: {{ bos_token }}{% for message in messages %}{{'<|' + message['role'] + '|>' + '\n",
      "' + message['content'] + '<|end|>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    del llm\n",
    "except:\n",
    "    pass\n",
    "\n",
    "llm = Llama(\n",
    "    model_path = model,\n",
    "    #*,\n",
    "\n",
    "    # Model Params\n",
    "    n_gpu_layers = 0,\n",
    "    split_mode = 1,\n",
    "    main_gpu = 0,\n",
    "    tensor_split = None,\n",
    "    vocab_only = False,\n",
    "    use_mmap = True,\n",
    "    use_mlock = False,\n",
    "    kv_overrides = None,\n",
    "\n",
    "    # Context Params\n",
    "    seed = 4294967295,\n",
    "    n_ctx = 0, #512,\n",
    "    n_batch = 512,\n",
    "    n_threads = None,\n",
    "    n_threads_batch = None,\n",
    "    rope_scaling_type = -1,\n",
    "    pooling_type = -1,\n",
    "    rope_freq_base = 0.0,\n",
    "    rope_freq_scale = 0.0,\n",
    "    yarn_ext_factor = -1.0,\n",
    "    yarn_attn_factor = 1.0,\n",
    "    yarn_beta_fast = 32.0,\n",
    "    yarn_beta_slow = 1.0,\n",
    "    yarn_orig_ctx = 0,\n",
    "    logits_all = False,\n",
    "    embedding = False,\n",
    "    offload_kqv = True,\n",
    "    flash_attn = False,\n",
    "\n",
    "    # Sampling Params\n",
    "    last_n_tokens_size = 64,\n",
    "\n",
    "    # Lora Params\n",
    "    lora_base = None,\n",
    "    lora_scale = 1.0,\n",
    "    lora_path = None,\n",
    "\n",
    "    # BAckend Params\n",
    "    numa = False,\n",
    "\n",
    "    # Chat Format Params\n",
    "    chat_format = None,\n",
    "    chat_handler = None,\n",
    "\n",
    "    # Speculative Decoding\n",
    "    draft_model = None,\n",
    "\n",
    "    # Tokenizer Override\n",
    "    tokenizer = None,\n",
    "\n",
    "    # KV cache quantization\n",
    "    type_k = None,\n",
    "    type_v = None,\n",
    "\n",
    "    # # Misc\n",
    "    verbose = True,\n",
    "\n",
    "    # # Extra Params\n",
    "    #**kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bab046bf-c62a-463d-b774-63395f011ada",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_Llama__backend_initialized',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_batch',\n",
       " '_c_tensor_split',\n",
       " '_candidates',\n",
       " '_create_completion',\n",
       " '_ctx',\n",
       " '_input_ids',\n",
       " '_mirostat_mu',\n",
       " '_model',\n",
       " '_n_ctx',\n",
       " '_n_vocab',\n",
       " '_scores',\n",
       " '_token_eos',\n",
       " '_token_nl',\n",
       " 'cache',\n",
       " 'chat_format',\n",
       " 'chat_handler',\n",
       " 'context_params',\n",
       " 'create_chat_completion',\n",
       " 'create_chat_completion_openai_v1',\n",
       " 'create_completion',\n",
       " 'create_embedding',\n",
       " 'ctx',\n",
       " 'detokenize',\n",
       " 'draft_model',\n",
       " 'embed',\n",
       " 'eval',\n",
       " 'eval_logits',\n",
       " 'eval_tokens',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'input_ids',\n",
       " 'kv_overrides',\n",
       " 'last_n_tokens_size',\n",
       " 'load_state',\n",
       " 'logits_to_logprobs',\n",
       " 'longest_token_prefix',\n",
       " 'lora_base',\n",
       " 'lora_path',\n",
       " 'lora_scale',\n",
       " 'metadata',\n",
       " 'model',\n",
       " 'model_params',\n",
       " 'model_path',\n",
       " 'n_batch',\n",
       " 'n_ctx',\n",
       " 'n_embd',\n",
       " 'n_threads',\n",
       " 'n_threads_batch',\n",
       " 'n_tokens',\n",
       " 'n_vocab',\n",
       " 'numa',\n",
       " 'pooling_type',\n",
       " 'reset',\n",
       " 'sample',\n",
       " 'save_state',\n",
       " 'scores',\n",
       " 'set_cache',\n",
       " 'set_seed',\n",
       " 'tensor_split',\n",
       " 'token_bos',\n",
       " 'token_eos',\n",
       " 'token_nl',\n",
       " 'tokenize',\n",
       " 'tokenizer',\n",
       " 'tokenizer_',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29f13a66-ebe7-4121-9b2a-371027800866",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     991.96 ms\n",
      "llama_print_timings:      sample time =       8.71 ms /    40 runs   (    0.22 ms per token,  4594.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =     991.89 ms /    11 tokens (   90.17 ms per token,    11.09 tokens per second)\n",
      "llama_print_timings:        eval time =    4509.53 ms /    39 runs   (  115.63 ms per token,     8.65 tokens per second)\n",
      "llama_print_timings:       total time =    5620.82 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olá! Como posso ajudá-lo hoje? Se tiver alguma pergunta ou necessidade, estou aqui para oferecer minha assistência.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Olá!\"\n",
    "\n",
    "messages = [\n",
    "            #{'role': 'system', 'content': ''},\n",
    "            # {'role': 'assistant', 'content': ''},\n",
    "            {'role': 'user', 'content': prompt},\n",
    "            # {'role': 'assistant', 'content': 'Follows answer in Chinese:'},\n",
    "           \n",
    "           ]\n",
    "\n",
    "output = llm.create_chat_completion(\n",
    "    messages=messages,\n",
    "    )\n",
    "\n",
    "print(output['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e7fa820-946a-44d7-9a18-0881de8a3b8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Ol\\xc3\\xa1!  Oi! Tudo bem? Como posso ajudar? Eu sou um modelo de intelig\\xc3\\xaancia artificial e estou aqui para responder suas perguntas ou ajudar com informa\\xc3\\xa7\\xc3\\xb5es. Vamos come\\xc3\\xa7ar?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.detokenize(llm._input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9fef60-df63-4f66-a3cb-683e97d036f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    1,     1,     3,  7780, 29588, 29576, 29473,     4,  1219,\n",
       "       29478, 29576,  1088, 10065, 28879, 29572, 25196,  1745,  1435,\n",
       "        1032, 14810,  1051, 29572, 28835,  6039,  3973,  2997, 29477,\n",
       "        1108, 10336,  1094, 16962, 19046,  1085,  1702,  1048, 11786,\n",
       "       29478,  3414,  3371,  1594,  1287,  1061,  1428, 29489,  3562,\n",
       "        1061,  4234,  1032, 14810, 29588, 29501,  1499,  1200,  5995,\n",
       "       29476, 11083, 29491,  1859,  5235,  1046,  1051,  1108, 24139,\n",
       "       10425,  1645,  3016, 26981, 17962, 29493,  2727,  5036, 29493,\n",
       "       12390,  1168, 10809,  1645, 26174,  1031,  1428, 29489,  3562,\n",
       "        1061, 29576,   781,   781, 29521,  1039, 19456, 29477, 29493,\n",
       "        1195,  8669, 29644,  5235,  8450, 15459,  1031,  3886, 26174,\n",
       "        1031,  6680,  2848,  3924, 29493,  4299, 25620, 12736,  1050,\n",
       "       11095,  1229,  3562,  1153,  3414, 17868,  1051,  1032,  9738,\n",
       "        2520,  1444, 29474,  5575, 29491,  1859,  5235,  1046,  1051,\n",
       "        1108, 24139, 10425,  1200, 17582, 10857,  1343,  1288,  1769,\n",
       "        8450, 29493,  1702,  1048, 11786, 29478,  3414,  1032, 14810,\n",
       "       29588, 29501,  1499, 29576], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm._input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25b68d97-79d8-4038-be10-659a09b54f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(llm._input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96fa198d-58d4-4808-9ec0-c5736934ce15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d4834b-2373-4577-aad3-fb926722a17c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82e1b38-0b15-4bfd-bc60-5129adc8530f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
