
![v](https://img.shields.io/badge/version-0.1.0-blue) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![v](https://img.shields.io/badge/updated-August%2018,%20%202024-green)

![banner](https://github.com/controlecidadao/samantha_ia/blob/main/images/banner.png)



## Samantha Interface Assistant: Experimental Environment Designed to Democratize the Use of Open Source Large Language Models (LLM)

### ‚ú® Welcome to Samantha: An Interface Assistant for Open Source Artificial Intelligence

<br>

üíª Samantha is just a simple interface assistant for open source artificial intelligence models, developed under [Open Science](https://www.unesco.org/en/open-science) principles (open methodology, open source, open data, open access, open peer review and open educational resources) for use on common Windows computers (without GPU). The program runs the LLM locally, free of charge and unlimitedly, without the need for an internet connection, except to download [GGUF](https://huggingface.co/docs/hub/gguf) models or when required by the execution of the code created by the models (g.e. to download datasets for data analysis). Its objective is to democratize knowledge about the use of AI and demonstrate that, using the appropriate technique, even small models are capable of producing responses similar to those of larger ones. Her mission is to help explore the boundaries of (realy) open AI models.

[Understanding LLMs: A Comprehensive Overview from Training to Inference](https://arxiv.org/pdf/2401.02038v2)

<br>

üïµÔ∏è‚Äç‚ôÄÔ∏è Samantha is being developed to assist in the exercise of social control of the public administration, considering the worrisome current scenario of the growing loss of citizens' trust in external control institutions, resulting from the destructuring of the technical staff of the courts of accounts. However, its features allow it to be used by anyone interested in exploring open-source artificial intelligence models, especially programmers.

<br>

‚ôæÔ∏è The system allows the sequential loading of a list of prompts ([prompt chaining](https://www.promptingguide.ai/techniques/prompt_chaining)) and models (model chaining), one model at a time to save memory, as well as the adjustment of their hyperparameters, allowing the response generated by the previous model to be feedbacked and analyzed by the subsequent model to generate the next response, in an unlimited number of interaction cycles between LLMs without human intervention. Models can only interact with the answer provided by the immediately preceding model, so each new response replaces the previous one. You can also use just one model and have it interact with its previous response over an unlimited number of text generation cycles.
<br><br>

üîó Some chaining examples **_without using_** Samantha's response **Feedback Loop** feature:

  * **(model_1) responds (prompt_1) X number of responses:** used to analyze model's deterministic and stochastic behavior with help of the Learning Mode, as well as to generate multiple diverse responses with stochastic settings.

  * **(model_1) responds (prompt_1, prompt_2, prompt_n):** used to execute multiples instructions sequencially with the same model (prompt chaining).

  * **(model_1, model_2, model_n) respond (prompt_1):** used to compare models' responses for the same single prompt (model chaining). Useful for comparing different models, as well as quantized versions of the same model.

  * **(model_1, model_2, model_n) respond (prompt_1, prompt_2, prompt_n):** used to compare models' responses for a list of prompts, as well as to execute a sequence of instructions using disctinct models (model and prompt chaining). Each model respond all prompts. In turn, when using the _Single Response per Model_ feature, each model respond to only one specific prompt.
<br><br>

üîó Some chaining examples **_using_** Samantha's response **Feedback Loop** feature:

  * **(model_1) responds (prompt_1) X number of responses:** Used to improve model's previous response through a fixed user instruction using the same model, as well as to generate a continuous dialog using a single model (model talking to itself).

  * **(model_1) responds (prompt_1, prompt_2, prompt_n):** used to improve model's previous response through multiples user instructions sequencially with the same model (prompt chaining). Each prompt is used to refine or complete the previous response, as well as to execute a sequence of prompts that depend on the previous response.

  * **(model_1, model_2, model_n) respond (prompt_1):** Used to improve previous model's response using disctinct models (model chaining), as well as to generate a dialog between different models.

  * **(model_1, model_2, model_n) respond (prompt_1, prompt_2, prompt_n):** Used to execute a sequence of instructions using disctinct models (model and prompt chaining) and _Single response per model_ feature.

Each of these models and prompts sequences can be executed more than once via the **_Number of loops_** feature.
<br><br>


üëâ **Chaining Sequence Template:   ( [models list] -> respond -> ( [user prompt list] X number of responses) ) X number of loops**
<br><br><br>

<p align="center">
  <a href="https://youtu.be/vt5fpE0bzSY">
    <img src="https://i.sstatic.net/Vp2cE.png" alt="Watch the video">
  </a>
</p>

<br>

üß© Sequencing of prompts and models allows the generation of long responses by fractioning the user input instruction. Every partial response fits in the model's response length defined in the model training process.
<br><br>

üîß As an open source tool for automatic self-interaction between AI models, Samantha Interface Assistant was designed to explore **reverse prompt engineering with self-improvement feedback loop** üîÅ. This technique helps small large language models (LLM) to generate more accurate responses by transferring to the model the task of creating the final prompt and corresponding response based on the user's initial imprecise instructions, adding intermediate layers to the prompt construction process. Samantha doesn't have a hidden system prompt like it does with proprietary models. All instructions are controlled by the user.
<br><br>

üé≤ Thanks to **emergent behavior**, with the right prompt and proper hyperparameter configuration, even small models working together can generate big responses!
<br><br>

> _The intelligence of the human species is not based on a single intelligent being, but based on a collective intelligence. Individually, we are actually not that intelligent or capable. Our society and economic system is based on having a vast range of institutions made up of diverse individuals with different specializations and expertise. This vast collective intelligence shapes who we are as individuals, and each of us follows our own path in life to become the unique individual, and in turn, contribute back to being part of our ever-expanding collective intelligence as a species. We believe that the development of artificial intelligence will follow a similar, collective path. **The future of AI will not consist of a single, gigantic, all-knowing AI system that requires enormous energy to train, run, and maintain, but rather a vast collection of small AI systems‚Äìeach with their own niche and specialty, interacting with each other, with newer AI systems developed to fill a particular niche**._ [Evolving New Foundation Models: Unleashing the Power of Automating Model Development - Sakana AI](https://sakana.ai/evolutionary-model-merge/)

<br>

üåé **A Small Step:** Samantha is just a movement towards a future where artificial intelligence is not a privilege but a tool for all in a world where individuals can leverage AI to enhance their productivity, creativity, and decision-making without barriers, walking a journey to democratize AI and make it a force for good in our daily lives.

<br>

‚ö†Ô∏è **Use Responsibly:**
The generated text reflects the content, biases, errors and improprieties present in their training datasets. We encourage responsible use of Samantha and for insights only, always keeping ethical considerations at the forefront of our interactions with AI algorithms, which are just complex mathematical models that generates coherent texts from the sequencing of words (tokens) based on the probability patterns extracted from the training texts.

<br>

ü¶æ **The Instrumental Nature of AI:** Recognizing the technological monopoly of artificial intelligence as a possible instrument of domination and the expansion of social inequalities represents a challenge at this inflection point in history. Noting the flaws of the smaller models during the text generation process aids in this understanding by comparing them with the claimed perfection of the larger proprietary models. It is necessary to reposition things in their proper places and question the romantic reductionist view of attributing human characteristics - such as intelligence (anthropomorphization caused by the psychological phenomenon of pareidolia) - to a technology produced by the human intellect. For this reason, it is essential to demystify artificial intelligence through a didactic approach to how this novel "word calculator" works. Certainly, the dopamine of the initial charm artificially created by the market will not withstand the generation of a few hundred tokens (token is the name given to the basic building block of texts that an LLM uses to understand and generate text. A token may be an entire word or part of a word).

<br>

‚úèÔ∏è **Text Generation Considerations:**
Users should be aware that the responses generated by AI are derived from the training of its large language models on a vast corpus of text data. The exact sources or processes used by the AI to generate its outputs cannot be precisely cited or identified. The content produced by the AI is not a direct quotation or compilation from specific sources. Instead, it reflects the patterns, statistical relationships, and knowledge that the AI's neural networks have learned and encoded during the training process on the broad data corpus. The responses are generated based on this learned knowledge representation, rather than being retrieved verbatim from any particular source material. While the AI's training data may have included authoritative sources, its outputs are its own synthesized expressions of the learned associations and concepts.

<br>

üéØ **Objective:**
The primary objective with Samantha is to **inspire** üí° others to create similar - and much better ones, to be sure - systems and to educate users on the utilization of AI. Our goal is to foster a community of developers and enthusiasts who can take the knowledge and tools to further innovate and contribute to the field of open source AI. By doing so, the aim to cultivate a culture of collaboration and sharing, ensuring that the benefits of AI are accessible to all, regardless of their technical background or financial resources. It is believed that by enabling more people to construct and comprehend AI applications, we can collectively drive progress and address **societal challenges** with informed and diverse perspectives. Let's work together to shape a future where AI is a **positive and inclusive force for humanity**. See [UNESCO's Ethics of Artificial Intelligence Recommendations](https://www.unesco.org/en/artificial-intelligence/recommendation-ethics?hub=32618).

<br>

üö® **The Human Cost of Innovation:**
While this system aims to empower users and democratize access to AI, it's crucial to acknowledge the ethical implications of this technology. The development of powerful AI systems often relies on the exploitation of human labor, particularly in data annotation and training processes. This can perpetuate existing inequalities and create new forms of digital divide. **As users of AI, we have a responsibility to be aware of these issues and advocate for fairer practices within the industry**. By supporting ethical AI development and promoting transparency in data sourcing, we can contribute to a more inclusive and equitable future for all.

  * [Como funciona o trabalho humano por tr√°s da intelig√™ncia artificial](https://www.youtube.com/watch?v=F0M9OH5n-hg)

  * [The "Modern Day Slaves" Of The AI Tech World](https://www.youtube.com/watch?v=VPSZFUiElls)

  * [Others sources](https://www.google.com/search?q=ai+labor+exploitation&tbm=nws)

<br>

üôè **On the Shoulders of Giants:**
Special thanks to Georgi Gerganov and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp) for making all of this possible, as well as to [Andrei Bleten](https://github.com/abetlen/llama-cpp-python) by his amazing Python bidings for the Gerganov C++ library ([llama-cpp-python](https://pypi.org/project/llama-cpp-python/)).

<br><br>





## üìå Samantha's Key Features

<details>
<summary>Features</summary>

<br><br>
‚úÖ **Open Source Foundation:** Built upon [Llama.cpp](https://github.com/ggerganov/llama.cpp) / [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) and [Gradio](https://www.gradio.app/) , under [MIT license](https://opensource.org/license/mit), Samantha runs on standard computers, even without a dedicated Graphics Processing Unit (GPU).<br><br>
  
‚úÖ **Offline Capability:** Samantha operates independently of the internet, requiring connectivity only for the initial download of model files or when required by the execution of the code created by the models. This ensures privacy and security for your data processing needs. Your sensitive data is not shared via the internet with companies through confidentiality agreements.<br><br>

‚úÖ **Unlimited and Free Use:** Samantha's open-source nature allows for unrestricted use without any costs or limitations, making it accessible to anyone, anywhere, anytime.<br><br>

‚úÖ **Extensive Model Selection:** With access to [thousands](https://huggingface.co/models?sort=trending&search=gguf) of foundation and fine-tuned open-source models, users can experiment with various AI capabilities, each tailored to different tasks and applications, allowing to chain the sequence of models that best meet your needs.<br><br>

‚úÖ **Copy and paste LLMs:** To try out a sequence of `gguf` models, just copy their download links from any Hugging Face repository and paste inside Samantha to run them right away in sequence.<br><br>

‚úÖ **Customizable Parameters:** Users have control over model hyperparameters such as **context window** length (_n_ctx_, _max_tokens_), **token sampling** (_temperature_, _tfs_z_, _top-k_, _top-p_, _min_p_, _typical_p_), **penalties** (_presence_penalty_, _frequency_penalty_, _repeat_penalty_) and **stop words** (_stop_), allowing for responses that suit specific requirements, with deterministic or stochastic behavior.<br><br>

‚úÖ **Interactive Experience:** Samantha's chaining functionality enables users to generate endless texts by chaining prompts and models, facilitating complex interactions between different LLMs without human intervention.<br><br>

‚úÖ **Feedback Loop:** This feature allows you to capture the response generated by the model and feed it back into the next cycle of the conversation.<br><br>

‚úÖ **Prompt List:** You can add any number of prompts (separated by `$$$` or `\n`) to control the sequence of instructions to be executed by the models. It is possible to import a TXT file with a predefined sequence of prompts.<br><br>

‚úÖ **Model List:** You can select any number of models and in any order to control which model responds to the next prompt.<br><br>

‚úÖ **Cummulative Response:** You can concatenate each new response by adding it to the previous response to be considered when generating the next response by the model. It is important to highlight that the set of concatenated responses must fit in the model's context window.<br><br>

‚úÖ **Learning Insights:** A feature called _Learning Mode_ lets users observe the model's decision-making process, providing insights into how it selects output tokens based on their probability scores (logits) and hyperparameter settings. A list of the least likely selected tokens is also generated.<br><br>

‚úÖ **Voice Interaction:** Samantha supports simple voice commands with offline speech-to-text [Vosk](https://alphacephei.com/vosk/) (English and Portuguese) and text-to-speech with SAPI5 voices, making it accessible and user-friendly.<br><br>

‚úÖ **Audio feedback:** The interface provides audible alerts to the user, signaling the beginning and end of the text generation phase by the model.<br><br>

‚úÖ **Document Handling:** The system can load small PDF and TXT files. Chaining user prompts, system prompt and model's URL list can be inputted via a TXT file for convenience.<br><br>

‚úÖ **Versatile Text Input:** Fields for prompt insertion allow users to interact with the system effectively, including system prompt, previous model response and user prompt to guide the model's response.<br><br>

‚úÖ **Code Integration:** Automatic extraction of Python code blocks from model's response, along with pre-installed [JupyterLab](https://jupyter.org/) integrated development environment (IDE) in an isolated virtual environment, enables users to run generated code swiftly for immediate results.<br><br>

‚úÖ **Edit, Copy and Run Python Code:** The system allows the user to edit the code generated by the model and run it by selecting, copying with `CTRL + C` and clicking the _Run code_ button. You can also copy a Python code from anywhere (e.g. from a webpage) and run it just by pressing _Copy Python code_ and _Run code_ buttons (as long as it uses the installed Python libraries).<br><br>

‚úÖ **Code Blocks Editing:** Users can select and run Python code blocks generated by the model that uses the libraries installed in the `jupyterlab` virtual environment by entering the `#IDE` comment in the output code, selecting and copying with `CTRL + C`, and finally clicking the _Run code_ button;<br><br>

‚úÖ **HTML output:** Display Python interpreter output in an HTML pop-up window when text printed in the terminal is other than '' (empty string). This feature allows, for example, to execute a script unlimitedly and only display the result when a certain condition is met;<br><br>

‚úÖ **Automatic Code Execution:** Samantha features the option to automatically run the Python code generated by the models sequentially.<br><br>

‚úÖ **Stop Condition:** Stops Samantha if the automatic execution of the Python code generated by the model prints a value other than '' (empty string) in the terminal. You can also force exit a running loop by creating a function that returns only the string `STOP_SAMANTHA` when a certain condition is met.<br><br>

‚úÖ **Incremental Coding:** Using deterministic settings, create Python code incrementally, making sure each part works before moving on to the next.<br><br>

‚úÖ **Complete access and control:** Through the ecosystem of Python libraries and the codes generated by the models, it is possible to access computer files, allowing you to read, create, change and delete local files, as well as access the internet, if available, to upload and download information and files.<br><br>

‚úÖ **Data Analysis Tools:** A suite of data analysis tools like [Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [SciPy](https://scipy.org/), [Scikit-Learn](https://scikit-learn.org/stable/index.html#), [Matplotlib](https://matplotlib.org/), [Seaborn](https://seaborn.pydata.org/), [Vega-Altair](https://altair-viz.github.io/), [Plotly](https://plotly.com/python/), [Bokeh](https://docs.bokeh.org/en/latest/index.html), [Dash](https://plotly.com/examples/), [Sweetviz](https://pypi.org/project/sweetviz/), [D-Tale](https://github.com/man-group/dtale), [DataPrep](https://dataprep.ai/), [NetworkX](https://networkx.org/), [Pyvis](https://pyvis.readthedocs.io/en/latest/index.html), [Selenium](https://selenium-python.readthedocs.io/), [PyMuPDF](https://pypi.org/project/PyMuPDF/) and [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) are available within JupyterLab for comprehensive analysis and visualization (for a complete list of all Python available libraries, use a prompt like **_"create a Python code that prints all modules installed. Separate each module with \<br\> tag."_** and press _Run code_ button. The result will be displayed in a browser popup). Integration with [DB Browser](https://sqlitebrowser.org/about/) is also available.<br><br>

‚úÖ **Performance Optimized:** To ensure smooth performance on CPUs, Samantha maintains a limited chat history to just the previous response, reducing the model's context window size to save memory and computational resources.<br>

</details>





<br><br>
## üõ†Ô∏è Installing Samantha

<details>
<summary>Instructions</summary>

<br><br>
To use Samantha you will need:
<br><br>
* Install [Visual Studio](https://visualstudio.microsoft.com/pt-br/vs/community/) (free community version) on your computer. Download it, run it, and select only the option **Desktop development with C++** (administrator privileges required):

  ![cmake](https://github.com/controlecidadao/samantha_ia/blob/main/images/cmake2.png)
<br><br>
* Download the zip file from Samantha's repository by clicking [here](https://github.com/controlecidadao/samantha_ia/archive/refs/heads/main.zip) and unzip it to your computer. Select the drive where you want to install the program:

   ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/directory.png)
<br><br>
* Open `samantha_ia-main` directory and double click on `install_samantha_ia.bat` file to start installation. Windows may ask you to confirm the origin of the `.bat` file. Click on 'More info' and confirm. We encorage to inspect the code of all files:

   ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/install.png)<br><br>

  >_This is the critical part of the installation. If everything goes well, the process will complete without displaying error messages in the terminal._<br>

  <br>
  
  The installation process takes about _**50 minutes**_ and should end with the creation of two virtual environments: `samantha`, to run just the AI ‚Äã‚Äãmodel, and `jupyterlab`, to run the other installed programs. It will take up about _**5 GB**_ of your hard drive.

<br>

* Once installed, open Samantha by double clicking on `open_samantha.bat` file. Windows may ask you again to confirm the source of the `.bat` file. This authorisation is required only the first time you run the program. Click on 'More info' and confirm:<br>

  ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/open_samantha.png)<br><br>

  A terminal window will open. This is the Samantha's **server-side**.

  After answering the initial questions (interface language and voice control options - voice control is not suitable for first use), the interface will open in a new browser tab. This is the Samantha's **browser-side**:

  <br>

  ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/interface_english.png)<br><br>

  With the browser window opened, Samantha is ready to go.

</details>





<br><br>
## üëü Testing a Model in 5 Steps

<details>
<summary>Instructions</summary>

<br><br>
Samantha needs just a `.gguf` model file to generate text. Follow these steps to perform a simple model test:
<br><br>

1) Open Windows Task Management by pressing `CTRL + SHIFT + ESC` and check available memory. Close some programs if necessary to free memory.

2) Visit [Hugging Face](https://huggingface.co/models?library=gguf&sort=trending&search=gguf) repository and click on the card to open the corresponding page. Locate the _Files and versions_ tab and choose a `.gguf` model that fits in your available memory.
   
4) Right click over the model download link icon and copy its URL.

5) Paste the model URL into Samantha's _Download models for testing_ field.

6) Insert a prompt into _User prompt_ field and press `Enter`. Keep the `$$$` sign at the end of your prompt. The model will be downloaded and the response will be generated using the default deterministic settings. You can track this process via Windows Task Management.

<br>

Every new model downloaded via this copy and paste procedure will replace the previous one to save hard drive space. Model download is saved as `MODEL_FOR_TESTING.gguf` in your _Downloads_ folder.

You can also download the model and save it permanently to your computer. For more datails, see the section below.

</details>





<br><br>
## ‚¨áÔ∏è Downloading Large Language Models (LLM)

<details>
<summary>Instructions</summary>

<br>

### Downloading Open Source Model Files (.gguf)

Open souce model can be downloaded from [Hugging Face](https://huggingface.co/models?sort=trending&search=gguf), using `gguf` as the search parameter. You can combine two words like `gguf code` or `gguf portuguese`.

You can also go to a specific repository and see all the `.gguf` models available for downloading and testing, like [https://huggingface.co/bartowski](https://huggingface.co/bartowski) or [NousResearch](https://huggingface.co/NousResearch).
<br><br>

The models are displayed on cards like this:

![model_card](https://github.com/controlecidadao/samantha_ia/blob/main/images/model_card.png)
<br><br>

To download the model, click on the card to open the corresponding page. Locate the **Model card** and **Files and versions** tabs:

![tabs](https://github.com/controlecidadao/samantha_ia/blob/main/images/tabs.png)
<br><br>

To download some models, you must agree to the terms of use.

After that, click on the **Files and versions** tab and download a model that fits in your available RAM space. To check your available memory, open Windows Task Manager by pressing `CTRL + SHIFT + ESC`, click on **Performance** tab (1) and select **Memory** (2):

<br>

![task](https://github.com/controlecidadao/samantha_ia/blob/main/images/task_manager.png)
<br><br>

We suggest to download the model with **Q4_K_M** (4-bit quantization) in its link name (put the mouse over the download button to view the complete file name in the link like this: `https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf?download=true`). As a rule, the larger the model size, the greater the accuracy of the generated text.
<br>

If the downloaded model doesn't fit into the available RAM space, your hard drive will be used, impacting performance.

Download the chosen model and save it to your computer or just copy the download link and paste it into Samantha's _Download model for testing_ field. Watch video tutorials in the section below for more details.
<br><br>

Note that each model has its own characteristics, presenting significantly different responses depending on its size, internal architecture, training method, predominant language of the training database, user prompt and hyperparameter adjustment, and it is necessary to test its performance for the desired task.

**Some models may not be loaded due to their technical characteristics or incompatibility with the current version of the [llama.cpp Python binding](https://github.com/abetlen/llama-cpp-python) used by Samantha**.

Where to find models to test: [Huggingface GGUF Models](https://huggingface.co/models?sort=trending&search=gguf)<br><br>

The quality of the responses generated by a model can be evaluated using some criteria, such as:

  * **Degree of understanding** of the explicit and implicit instructions contained in the user and system prompts;

  * **Degree of obedience** to these instructions, aspect related to the predominant language of the database;

  * **Degree of hallucination** in the generation of coherent text, but incorrect or out of context. Hallucination in text generation typically results from insufficient training of the model or inappropriate selection of the next token, which leads the model in an undesired semantic direction;

  * **Degree of precision in the decision-making process** to fill in the gaps in the context of the user prompt and to resolve ambiguities necessary to generate the response. What is not explicitly specified, the model tries to infer based on its training, which can lead to errors;

  * **Degree of coherence of the bias adopted by the model** with the bias (or lack thereof) contained in the user's prompt;

  * **Degree of pertinence and relevance of the topics choosen** to be addressed;

  * **Degree of breadth and depth of approach to topics** in the response;

  * **Degree of syntatic and semantic precision** of the response;

  * **Quality of the structure and content of the response** in relation to the user's expectations (and their overcoming) for the problem submitted to the model, considering the technique used to create the prompt (prompt engineering) and the adjustment of the model's hyperparameters.
</details>





<br><br>
## üß† Samantha's Controls & Settings - üöß Under Construction

### Interface Left Column (input):
<br>

**Main Controls:**
<details>
<summary>Start chat (button)</summary>
 
---
 
‚Äãüì£‚Äã Starts a chat session, sending all input texts (system prompt, assistant previous response and user prompt) to the server, as well as the settings adjusted by the user. Just like all other buttons, a mouse click will sound.

A chat session can contain more than one conversation cycle (loop).

_Start chat_ button keyboard shortcut: Press `Enter` anywhere on the page.

To generate text, a model must be pre-selected in _Model selection_ dropdown list or a Hugging Face model URL must be provided to _Download model for testing_ field. If both fields are filled in, the model selected via the dropdown list takes precedence.

---

<br><br>
</details>

<details>
<summary>Stop / Next (button)</summary>

---

üõë‚Äã Interrupts the token generation process for the current model, starting the execution of the next model in the sequence, if any.

It also stops playback of the currently playing audio when in speech autoplay mode (_Read response aloud_ checkbox selected).

Samantha has 3 phases:<br>
1) Loading model (non stop)
2) Thinking (non stop)
3) Next token selection (stop).

This button works only when the next token selection phase is started, even if it was pressed previously.

---

<br><br>
</details>

<details>
<summary>Clean history (button)</summary>

---

üßπ‚Äã Clears the history of the current chat session, erasing the assistant output field as well as all internal logs, previous response etc.

---

<br><br>
</details>

<details>
<summary>Load model (button)</summary>

---

‚úÖ‚Äã Allows you to select the directory where the models available for loading are saved.

Default: Windows "Downloads" folder

You can select any directory that contains `GGUF` models. In this case, the models contained in the selected directory will be listed in the _Model selection_ dropdown list.

When the pop-up window opens, make sure to click on the folder you want to select.

---

<br><br>
</details>

<details>
<summary>Stop all & reset (button)</summary>

---

üõë Stops the sequence of running models and resets internal settings of the last loaded model.

After resetting, models take some time to restart text generation, depending on the size of the input text.

---

<br><br>
</details>

<details>
<summary>Replace response (button)</summary>

---

üìë Replaces the text in the _Assistant previous response_ field with the text of the last response generated by the model. 

The replaced text will be used as the model's previous response in the next conversation cycle. 

This replaced text is not visible. It does not erase text from the _Previous Assistant Response_ field, which can be used again later.

---

<br><br>
</details>


<details>
<summary>System prompt (textbox)</summary>

---

üíª‚Äã In the context of Large Language Models (LLMs), a system prompt is a special type of instruction given to the model at the beginning of a conversation or task. It is considered in all interactions with the model.

Think of it as setting the stage for the interaction. It provides the LLM with crucial information about its role, the desired persona, behavior, and the overall context of the conversation.

Here's how it works:

1. Defining the Role: The system prompt clearly defines the LLM's role in the interaction. 
    * For example, it might instruct the model to act as a helpful assistant, a creative writer, a factual summarizer, or even a character in a story.

2. Setting the Tone and Persona:  The system prompt can also establish the desired tone and persona for the LLM's responses. 
    * It could be formal, informal, humorous, serious, or any other style depending on the intended use case.

3. Providing Contextual Information: The system prompt can offer background information relevant to the conversation or task. 
    * This helps the LLM understand the user's needs and provide more accurate and relevant responses.

Benefits of Using System Prompts:

* Improved Consistency: System prompts ensure that the LLM consistently adheres to a specific role and style throughout the interaction.
* Enhanced Accuracy: By providing context and instructions, system prompts help the LLM generate more accurate and relevant responses.
* Tailored Experiences: Different system prompts can be used to create tailored experiences for users based on their needs and preferences.

Example:

Let's say you want to use an LLM to write a poem in the style of Shakespeare. A suitable system prompt would be:

```
You are William Shakespeare, a renowned poet from Elizabethan England.
```

By providing this system prompt, you guide the LLM to generate a response that reflects Shakespeare's language, style, and thematic interests.

Not all models support system prompt. Test to find out: fill in "x = 2" in the _System prompt_ field and ask the model the value of "x" in the _User prompt_ field. If the model gets the value of "x", system prompt is available in the model.

You can simulate the effect of the system prompt by adding text in square brackets in the beginning of the _User prompt_ field: ```[This text acts as a system prompt]``` or adding the system prompt text into the _Assistant previous response_ field (do not use feedback loop).

---

<br><br>
</details>

<details>
<summary>Feedback loop (checkbox)</summary>

---

‚Ü©Ô∏è‚Äã When activated, it automatically considers the response generated by the model in the current conversation cycle as being the Assistant's previous response in the next cycle, allowing feedback from the system. 

Any text entered by the user in the _Assistant previous response_ field is only considered in the first cycle after activating this feature. In the following cycles, the model's response internally replaces the previous response, but without deleting the text contained in that field, which can be reused in a new chat session. You can monitor the content of the assistant previous response via terminal.

In turn, when deactivated, it always uses the text contained in the _Assistant previous response_ field as the previous response, unless the text is preceded by `---` (triple dash). Text preceded by `---` is ignored by the model.

To internally clear the model's previous response, press the _Clean history_ button.

---

<br><br>
</details>

<details>
<summary>Assistant previous response (textbox)</summary>

---

‚û°Ô∏è‚Äã Stores the text considered by the model as its previous response in the current conversation cycle.

Used to feed back the responses generated by the model.

To ignore the text present in this field, include `---` at the beginning.

---

<br><br>
</details>

<details>
<summary>User prompt (textbox)</summary>

---

‚úèÔ∏è‚Äã The main input field of the interface. It receives the list of user prompts that will be submitted to the model sequentially.

Each item in the list must be separated from the next one by a line break (`SHIFT + ENTER` or `\n`) or by the symbols `$$$` (triple dollar signal), if the items are made up of text with line breaks.

When present in the user prompt, the `$$$` separator takes precedence over the `\n` separator. In other words, `\n` is ignored.

You can import a TXT file containing a list of prompts.

`---` before a prompt list item causes the system to ignore that item.

Text positioned within single square brackets (`[` and `]`) is added to the beginning of each prompt list item, simulating a system prompt.

Text positioned within double square brackets (`[[` and `]]`) is added as the last item in the prompt list. In this case, all responses generated by the model in the current chat session are concatenated and added to the end of this item, allowing the model to analyze them together.

If the Python code execution returns only the word `STOP_SAMANTHA`, it stops token generation and exits the loop.

If the Python code execution returns only `''` (empty string), it does not display the HTML pop-up window.

Example:<br>

_[You are a poet that writes only in Portuguese]_<br>
_Create a sentence about love_<br>
_Create a sentence about life_<br>
---_Create a sentence about time (this instruction is ignored)_<br>
_[[Create a paragraph in English that summarizes the ideas contained in the following sentences:]]_<br>
(_previous responses are concatenated here_)<br>

Model responses sequence:<br>

_"O amor √© um fogo que arde no meu peito, uma chama que me guia atrav√©s da vida."_<br>
_"A vida √© um rio que flui sem parar, levando-nos para al√©m do que conhecemos."_<br>
_Love and life are intertwined forces that shape our existence. Love burns within us like a fire, guiding us through life's journey with passion and purpose. Meanwhile, life itself is a dynamic and ever-changing river, constantly flowing and carrying us beyond the familiar and into the unknown. Together, love and life create a powerful current that propels us forward, urging us to explore, discover, and grow._

---

<br><br>
</details>

<details>
<summary>Model selection (dropdown)</summary>

---

‚úÖ ‚ÄãDropdown list of models saved on the computer and available for text generation.

To view models in this field, click the _Load model_ button and select the folder containing the models.

The default location for saving models is the Windows _Downloads_ directory.

You can select multiples models (even repeated) to create a sequence of models to respond the user prompts.

The last model downloaded from a URL is saved as `MODEL_FOR_TESTING.gguf` and is also displayed in this list.

---

<br><br>
</details>

<details>
<summary>Download model for testing (textbox)</summary>

---

‚Äã‚¨áÔ∏è‚Äã Receives a list of Hugging Face links to the models that will be downloaded and executed sequencially.

Link example: 

* https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf?download=true

Links preceded by `---` will be ignored.

Only works if no model is selected in _Model selection_ dropdown list.

---

<br><br>
</details>

<details>
<summary>Single response per model (checkbox)</summary>

---

‚Äã1Ô∏è‚É£‚Äã Activates a single response per model. 

Prompts that exceed the number of models are ignored.

Models that exceed the number of prompts are also ignored.

You can select the same model more than once.

This checkbox disables _Number of loops_ and _Number of responses_ checkboxes.

---

<br><br>
</details>

<details>
<summary>Reset model (checkbox)</summary>

---

‚Äã‚Äã‚Äã‚èÆÔ∏è Reinitializes the internal state of the model, eliminating the influence of the previous context.

How it Works:

When the reset feature is invoked:

* The model's internal state related to the current context is cleared.
* Any accumulated tokens from previous interactions are discarded.
* The model essentially returns to its initial state, as if it was just loaded.

Benefits:

* Improved consistency: Each new interaction starts fresh, reducing the chance of the model being influenced by unrelated previous context.
* Better control: Users can manage when the model should "forget" previous interactions.

Use Cases:

* In chatbots or conversational AI where you want to start new chat sessions cleanly.
* In applications processing multiple independent text generation tasks.
* When fine-tuning or testing the model's behavior under controlled conditions.

---

<br><br>
</details>

<details>
<summary>Shuffle models (checkbox)</summary>

---

üé∞‚Äã Shuffles the execution order of the models if 3 or more models are selected in _Model selection_ dropdown list.

---

<br><br>
</details>

<details>
<summary>Fast mode (checkbox)</summary>

---

üèÉ‚Äç‚ôÄÔ∏è‚Äã Generates text faster in the background without displaying the addition of each token in the _Assistant output_ field.

Minimizing or hiding the Samantha browser window makes the token generation process even faster.

This checkbox disables Learning Mode.

---

<br><br>
</details>

<details>
<summary>Voice selection (dropdown)</summary>

---

üó£Ô∏è‚Äã Selects the language of the computer's SAPI5 voice that will read the responses generated by the model.

---

<br><br>
</details>

<details>
<summary>Read response aloud (checkbox)</summary>

---

üé∂‚Äã Activates automatic reading mode for responses generated by the model using the language selected in the _Voice selection_ dropdown list.

---

<br><br>
</details>

<details>
<summary>Learning mode (radio buttons)</summary>

---

‚Äãüë©‚Äçüè´ ‚ÄãActivates Learning Mode.

It presents a series of features that help in understanding the token selection process by the model, such as:

* Model metadata
* Tokens vocabulary
* Top-k tokens sorted by logits score with token position in the vocabulary and selected token indication
* Barplot of the top-k tokens sorted by logits scores
* Cummulative barplot of the selected unlikely tokens

Only works if _Fast Mode_ is unchecked. 

Radio buttons options:<br>

* OFF: Learning mode disabled.<br>
* 0, 0.3, 1, 3, 10: Generation time delay in seconds.<br>
* NEXT TOKEN: Allows you to control the response generation process, token by token, via _NEXT TOKEN_ button.<br>

---

<br><br>
</details>

<details>
<summary>Number of loops (radio buttons)</summary>

---

üîÇ‚Äã Set the number of repetitions of the block in the following chaining sequence:

Chaining Sequence: ( [models list] -> respond -> ( [user prompt list] X number of responses) ) **X _number of loops_**

Each model in the _models list_ responds to all prompts in the _user prompt list_ for the selected _number of responses_. This block is repeated for the selected _number of loops_.

---

<br><br>
</details>

<details>
<summary>Number of responses (radio buttons)</summary>

---

üîÇ Number of responses to be generated by each selected model in the following chaining sequence:

Chaining Sequence: ( [models list] -> respond -> ( [user prompt list] **X _number of responses_**) ) X number of loops

Each model in the _models list_ responds to all prompts in the _user prompt list_ for the selected _number of responses_. This block is repeated for the selected _number of loops_.


---

<br><br>
</details>

<details>
<summary>Run code automatically (checkbox)</summary>

---

üë©‚Äçüíª‚Äã When checked, runs automatically the Python code generated by the model.

Whenever Python code returns a value other than `''` (empty string), an HTML pop-up window opens to display the returned content.

---

<br><br>
</details>

<details>
<summary>Stop condition (checkbox)</summary>

---

‚Äãüõë‚Äã When checked, stops Samantha when the automatic execution of the Python code generated by the model prints a value other than `''` (empty string) in the terminal.

Use it to stop a generation loop when a condition is met.

---

<br><br>
</details>

<details>
<summary>Cummulative response (checkbox)</summary>

---

‚Äãüì•‚Äã When checked, concatenates each new response by adding it to the previous response to be considered when generating the next response by the model. 

It is important to highlight that the set of concatenated responses must fit in the model's context window.

---

<br><br>
</details>

<br><br>
**Context Window:**

<details>
<summary>n_ctx (slider)</summary>

---

üìñ‚Äã `n_ctx` stands for _number of context tokens_ in the context window and determines the maximum number of tokens that the model can process at once. It determines how much previous text the model can "remember" and utilize when selecting the next token from model vocabulary.

The context length directly impacts the memory usage and computational load. Longer `n_ctx` requires more memory and computational power.

How `n_ctx` works:

It sets the upper limit on the number of tokens the model can "see" at once.
Tokens are usually word parts, full words, or characters, depending on the tokenization method.
The model uses this context to understand and generate text.
For example, if `n_ctx` is 2048, the model can process up to 2048 tokens (now words) at a time.

Impact on model operation:

During training and inference, the model attends to all tokens within this context window.<br>
It allows the model to capture long-range dependencies in the text.<br>
Larger `n_ctx` enables the model to handle longer sequences of text without losing earlier context.<br>

Why increasing `n_ctx` increases memory usage:

Attention mechanism: LLMs uses self-attention mechanisms (like in Transformers) which compute attention scores between all pairs of tokens in the input.<br>
Quadratic scaling: The memory required for attention computations scales quadratically with the context length. If you double `n_ctx`, you quadruple the memory needed for attention.<br>

**CAUTION: `n_ctx` MUST BE GREATER THAN (`max_tokens` + number of input tokens)** (system prompt + assistant previous response + user prompt).

When set to `0`, the system will use the maximum `n_ctx` possible (model's context window size).<br>
As a rule, set `n_ctx` equal to `max_tokens`, but only to the value necessary to accommodate the text parsed by the model. Samantha's default values ‚Äã‚Äãfor `n_ctx` and `max_tokens` are 4,000 tokens.<br>
Before adjusting `n_ctx`, you must to unload the model by clicking _Unload model_ button.

Example:

User prompt = 2000 tokens<br>
`n_ctx`= 4000 tokens<br>
If the text generated by the model is equals or greater than 14 tokens (4000 - 2000), the system will raise an `IndexError` in the terminal, but the interface will not crash.

To check the impact of the `n_ctx` in memory, open Windows Task Manager (`CTRL + SHIFT + ESC`) to monitor memory usage, select memory panel and vary `n_ctx` values. Don't forget to unload model between changes.

---

<br><br>
</details>

<details>
<summary>max_tokens (slider)</summary>

---

üéöÔ∏è‚Äã Controls maximum number of tokens to be generated by the model.

Select `0` for the models' maximum number of tokens (maximum memory required).

How `max_tokens` Works:

1. Sampling Process: When generating text, LLMs predict the next token based on the context provided (system prompt + previous response + user prompt + text already generated). This prediction involves calculating probabilities for each possible token in the vocabulary.
   
2. Token Limit: The `max_tokens` parameter sets a hard limit on how many tokens the model can generate before stopping, regardless of the predicted probabilities.

3. Truncation: Once the generated text reaches `max_tokens`, the generation process is abruptly terminated. This means the final output might be incomplete or feel cut off.

---

<br><br>

</details>

<br><br>
**Stop Words:**

<details>
<summary>stop (textbox)</summary>

---

üî§‚Äã List of characters that interrupt text generation by the model, in the format `["$$$", ".", ".\n"]` (Python list).

---

<br><br>
</details>

<br><br>
**Token Sampling:**

<details>
<summary>General instructions</summary>

---

**Deterministic Behavior:**

To check the deterministic impact of each hyperparameter on the model's behavior, set all others hyperparameters to their maximum stochastic values ‚Äã‚Äãand execute a prompt more than once. Repeat this procedure for each token sampling hyperparameter.
<br><br>

**Stochastic Behavior:**

To check the stochastic reflection of a hyperparameter on the model's behavior, set all other hyperparameters to their maximum stochastic values ‚Äã‚Äãand gradually vary the selected hyperparameter based on its deterministic value. Repeat this procedure for each token sampling hyperparameter.

You can combine stochastic tuning of different hyperparameters.
<br><br>


| Hyperparameter | Deterministic    | Stochastic    |
| :------------: | :--------------: | :-----------: |
| _temperature_  | 0                | > 0           |
| _tfs_z_        | 0                | > 0           |
| _top_p_        | 0                | > 0           |
| _min_p_        | 1                | < 1           |
| _typical_p_    | 0                | > 0           |
| _top_k_        | 1                | > 1           |


---

<br><br>
</details>


<details>
<summary>temperature (slider)</summary>

---

üå° Temperature is a hyperparameter that controls the randomness of the text generation process in LLMs. It affects the probability distribution of the model's next-token predictions.
<br><br>

>_Temperature is a hyperparameter t that we find in stochastic models to regulate the randomness in a sampling process (Ackley, Hinton, and Sejnowski 1985). The softmax function (Equation 1) applies a non-linear transformation to the output logits of the network, turning it into a probability distribution (i.e. they sum to 1). The temperature parameter regulates its shape, redistributing the output probability mass, flattening the distribution proportional to the chosen temperature. This means that for t > 1, high probabilities are decreased, while low probabilities are increased, and vice versa for t < 1. Higher temperatures increase entropy and perplexity, leading to more randomness and uncertainty in the generative process. Typically, values for t are in the range of [0, 2] and t = 0, in practice, means greedy sampling, i.e. always taking the token with the highest probability._
_[Is Temperature the Creativity Parameter of Large Language Models?](https://arxiv.org/pdf/2405.00492)_

<br>

[The Effect of Sampling Temperature on Problem Solving in Large Language Models](https://arxiv.org/pdf/2402.05201)

<br>

**Controlling Creativity:**

üî• Use **higher temperatures** when you want the model to generate more creative, unexpected, and varied responses. This is useful for creative writing, brainstorming, and exploring multiple ideas.<br>
This flattens the probability distribution, making the model more likely to sample less probable tokens.<br>
The generated text becomes more diverse and creative, but potentially less coherent.<br>

‚ùÑ Use **lower temperatures** when you need more predictable and focused output. This is useful for tasks requiring precise and reliable information, such as summarization or answering factual questions.<br>
This sharpens the probability distribution, making the model more likely to sample the most probable tokens.<br>
The generated text becomes more focused and deterministic, but potentially less creative.
<br><br>

**How it works:**

üßÆ ‚ÄãMathematically, the temperature (T) is applied by dividing the logits (raw scores from the model) by T before applying the softmax function.<br>
A lower temperature makes the distribution more "peaked," favoring high-probability options.<br>
A higher temperature "flattens" the distribution, giving more chance to lower-probability options.<br>
<br>

**Temperature scale:**

Generally ranges from 0 to 2, with 1 being the default (no modification).<br>
T < 1: Makes the text more deterministic, focused, and "safe."<br>
T > 1: Makes the text more random, diverse, and potentially more creative.<br>
T = 0: Equivalent to greedy selection, always choosing the most probable option.<br>
<br>

**Avoiding Repetition:**

Higher temperatures can help reduce repetitive patterns in the generated text by promoting diversity.<br>
Very low temperatures can sometimes lead to repetitive and deterministic outputs, as the model might keep choosing the highest-probability tokens.<br>

It's important to note that temperature is just one of several sampling hyperparameters available. Others include top-k sampling, nucleus sampling (or top-p), and the TFS-Z. Each of these methods has its own characteristics and may be more suitable for different tasks or generation styles.
<br><br>

**Videos:**

[temperature shorts 1](https://www.youtube.com/shorts/XsLK3tPy9SI)

[temperature shorts 2](https://www.youtube.com/shorts/0Bw95ILozjY)

---

<br><br>
</details>

<details>
<summary>tfs_z (slider)</summary>

---

`tfs_z` stands for **tail-free sampling with z-score**. It's a hyperparameter used in a text generation technique designed to balance the trade-off between diversity and quality in generated text.
<br><br>

**Context and purpose:**

Tail-free sampling was introduced as an alternative to other sampling methods like `top-k` or nucleus (`top-p`) sampling. Its goal is to remove the arbitrary "tail" of the probability distribution while maintaining a dynamic threshold.
<br><br>

**Technical Details of `tfs_z` in LLM Text Generation**
<br><br>

**Probability distribution analysis:**

The method examines the probability distribution of the next token predictions.
It focuses on the "tail" of this distribution - the less likely tokens.
<br><br>

**Z-score calculation:**

For each token in the sorted (descending) probability distribution, a z-score is calculated.
The z-score represents how many standard deviations a token's probability is from the mean.
<br><br>

**Cutoff determination:**

The `tfs_z` parameter sets the z-score threshold.
Tokens with a z-score below this threshold are removed from consideration.
<br><br>

**Dynamic thresholding:**

Unlike fixed methods like `top-k`, the number of tokens retained can vary based on the shape of the distribution.
This allows for more flexibility in different contexts.
<br><br>

**Sampling process:**

After applying the `tfs_z` cutoff, sampling occurs from the remaining tokens.
This can be done using various methods (e.g., temperature-adjusted sampling).

`tfs_z` is a hyperparameter that controls the **temperature scaling** of the output logits during text generation.
<br><br>

**Here's what it does:**

1. **Logits**: When an LLM generates text, it produces a probability distribution over all possible tokens in the vocabulary. This distribution is represented as a vector of logits (unnormalized log probabilities).
   
2. **Temperature scaling**: To control the level of uncertainty or "temperature" of the output, you can scale the logits by multiplying them with a temperature factor (`t`). This is known as temperature scaling.

3. **`tfs_z` hyperparameter**: It's a hyperparameter that controls how much to scale the logits before applying temperature scaling.

When you set `tfs_z > 0`, the model first normalizes the logits by subtracting their mean (`z-score normalization`) and then scales them with the temperature factor (`t`). This has two effects:

* **Reduced variance**: By normalizing the logits, you reduce the variance of the output distribution, which can help stabilize the generation process.
  
* **Increased uncertainty**: By scaling the normalized logits with a temperature factor, you increase the uncertainty of the output distribution, which can lead to more diverse and creative text generations.
<br><br>

**Practical example:**

Imagine that the model is trying to complete the sentence "The sky is..."<br>

Without `tfs_z`, the model could consider:<br>
blue (30%), cloudy (25%), clear (20%), dark (15%), green (5%), singing (3%), salty (2%)

With TFS-Z (cut by 10%):<br>
blue (30%), cloudy (25%), light (20%), dark (15%)

This eliminates less likely and potentially meaningless options, such as "The sky is salty."

By adjusting the Z-score, we can control how "conservative" or "creative" we want the model to be. A higher Z-score will result in fewer but more "safe" options, while a lower Z-score will allow for more variety but with a greater risk of inconsistencies.

In summary, `tfs_z` controls how much to scale the output logits after normalizing them. A higher value of `tfs_z` will produce more uncertain and potentially more creative text generations.

Keep in mind that this is a relatively advanced hyperparameter, and its optimal value may depend on the specific LLM architecture, dataset, and task at hand.

---

<br><br>
</details>

<details>
<summary>top_p (slider)</summary>

---

‚≠ï‚Äã `Top-p` (nucleus sampling) is a hyperparameter that controls the diversity and quality of text generation in LLMs. It affects the selection of tokens during the generation process by dynamically limiting the vocabulary based on cumulative probability.
<br><br>

**Controlling Output Quality:**

üîÆ Use **higher `top-p` values** (closer to 1) when you want the model to consider a wider range of possibilities, potentially leading to more diverse and creative outputs. This is useful for open-ended tasks, storytelling, or generating multiple alternatives. Higher values allow for more low-probability tokens to be included in the sampling pool.

üéØ Use **lower `top-p` values** (closer to 0) when you need more focused and high-quality output. This is beneficial for tasks requiring precise information or coherent responses, such as answering specific questions or generating formal text. Lower values restrict the sampling to only the most probable tokens.
<br><br>

**How it works:**

üßÆ Mathematically, `top-p` sampling selects the smallest possible set of words whose cumulative probability exceeds the chosen p-value. The model then samples from this reduced set of tokens. This approach adapts to the confidence of the model's predictions, unlike fixed methods like `top-k` sampling.
<br><br>

**Top-p scale:**

Generally ranges from 0 to 1, with common values between 0.1 (10% most likely) and 0.9 (90% most likely).<br>
p = 1: Equivalent to unmodified sampling from the full vocabulary.<br>
p ‚Üí 0: Increasingly deterministic, focusing on the highest probability tokens.<br>
p = 0.9: A common choice that balances quality and diversity.
<br><br>

**Balancing Coherence and Diversity:**

`Top-p` sampling helps maintain coherence while allowing for diversity. It adapts to the model's confidence, using a smaller set of tokens when the model is very certain and a larger set when it's less certain. This can lead to more natural-sounding text compared to fixed cutoff methods.
<br><br>

**Comparison with Temperature:**

While temperature modifies the entire probability distribution, `top-p` directly limits the vocabulary considered. Top-p can be more effective at preventing low-quality outputs while still allowing for creativity, as it dynamically adjusts based on the model's confidence.

It's worth noting that `top-p` is often used in combination with other sampling methods, such as `temperature` adjustment or `top-k` sampling. The optimal choice of hyperparameters often depends on the specific task and desired output characteristics.

---

<br><br>
</details>

<details>
<summary>min_p (slider)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>typical_p (slider)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>top_k (slider)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<br><br>
**Token Penalties:**

<details>
<summary>General instructions</summary>

---

üß≠ Syntactic and semantic variation arises from the penalization of tokens that are replaced by others that begin words related to different ideas, leading the response generated by the model in another direction.

Syntactic variations do not always generate semantic variations.

As text is generated, penalties become more frequent as there are more tokens to be punished.

**Deterministic Behavior:**

To obtain a deterministic text (same input, same output), but without repeating words (tokens), increase the values ‚Äã‚Äãof the penalty hyperparameters.

However, if it proves necessary to allow the model to reselect already generated tokens, keep these settings at their default values.
<br><br>

| Hyperparameter      | Default Values   | Text Diversity |
| :-----------------: | :--------------: | :------------: |
| _presence_penalty_  | 0                | > 0            |
| _frequency_penalty_ | 0                | > 0            |
| _repeat_penalty_    | 1                | > 1            |

---

<br><br>
</details>


<details>
<summary>presence_penalty (slider)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>frequency_penalty (slider)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>repeat_penalty (slider)</summary>

---

**How `repeat_penalty` works:**

üìä Starting from the default value (=1), as we reduce this value (<1) the text starts to present more and more repeated words (tokens), to the point where the model starts to repeat a certain passage or word indefinitely.

In turn, as we increase this value (>1), the model starts to penalize repeated words (tokens) more heavily, up to the point where the input text no longer generates penalties in the output text.

During the penalty process (>1), there is a variation in syntactic and semantic coherence.

Practical observations showed that increasing the token penalty (>1) generates syntactic and semantic diversity in the response, as well as promoting a variation in the response length until stabilization, when increasing the value no longer generates variation in the output.

The `repeat_penalty` hyperparameter has a deterministic nature.
<br><br>

**Adjustment tip:**

üìà increase this value until the generated text reaches the desired level of diversity and does not present syntactic errors (depends on the model).

---

</details>

<br><br>
**Others:**

<details>
<summary>Model metadata (textbox)</summary>

---

Displays model's metadata.
<br><br>


**Example:**

Model: https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf?download=true
<br><br>

```
{'general.name': 'Hermes 3 Llama 3.1 8B'
  'general.architecture': 'llama'
  'general.type': 'model'
  'general.organization': 'NousResearch'
  'llama.context_length': '131072'
  'llama.block_count': '32'
  'general.basename': 'Hermes-3-Llama-3.1'
  'general.size_label': '8B'
  'llama.embedding_length': '4096'
  'llama.feed_forward_length': '14336'
  'llama.attention.head_count': '32'
  'tokenizer.ggml.eos_token_id': '128040'
  'general.file_type': '7'
  'llama.attention.head_count_kv': '8'
  'llama.rope.freq_base': '500000.000000'
  'llama.attention.layer_norm_rms_epsilon': '0.000010'
  'llama.vocab_size': '128256'
  'llama.rope.dimension_count': '128'
  'tokenizer.ggml.model': 'gpt2'
  'tokenizer.ggml.pre': 'llama-bpe'
  'general.quantization_version': '2'
  'tokenizer.ggml.bos_token_id': '128000'
  'tokenizer.ggml.padding_token_id': '128040'
  'tokenizer.chat_template': "{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"}
```

---

<br><br>
</details>

<details>
<summary>Show model's vocabulary (checkbox)</summary>

 ---

‚úÖ Displays model's metadata when selected.

---

<br><br>
</details>




<details>
<summary>Model's vocabulary (textbox)</summary>

 ---

üî† Displays model's vocabulary when _Learning Mode_ and _Show Model Vocabulary_ are selected simultaneously.

**Example (from token 2000 to 2100):**

Model: https://huggingface.co/Triangle104/Mistral-7B-Instruct-v0.3-Q5_K_M-GGUF/resolve/main/mistral-7b-instruct-v0.3-q5_k_m.gguf?download=true
<br><br>


```
2000)    'ility'
2001)    ' √©'
2002)    ' er'
2003)    ' does'
2004)    ' here'
2005)    'the'
2006)    'ures'
2007)    ' %'
2008)    'min'
2009)    ' null'
2010)    'rap'
2011)    '")'
2012)    'rr'
2013)    'List'
2014)    'right'
2015)    ' User'
2016)    'UL'
2017)    'ational'
2018)    ' being'
2019)    'AN'
2020)    'sk'
2021)    ' car'
2022)    'ole'
2023)    ' dist'
2024)    'plic'
2025)    'ollow'
2026)    ' pres'
2027)    ' such'
2028)    'ream'
2029)    'ince'
2030)    'gan'
2031)    ' For'
2032)    '":'
2033)    'son'
2034)    'rivate'
2035)    ' years'
2036)    ' serv'
2037)    ' made'
2038)    'def'
2039)    ';\r'
2040)    ' gl'
2041)    ' bel'
2042)    ' list'
2043)    ' cor'
2044)    ' det'
2045)    'ception'
2046)    'egin'
2047)    ' –±'
2048)    ' char'
2049)    'trans'
2050)    ' fam'
2051)    ' !='
2052)    'ouse'
2053)    ' dec'
2054)    'ica'
2055)    ' many'
2056)    'aking'
2057)    ' √†'
2058)    ' sim'
2059)    'ages'
2060)    'uff'
2061)    'ased'
2062)    'man'
2063)    ' Sh'
2064)    'iet'
2065)    'irect'
2066)    ' Re'
2067)    ' differ'
2068)    ' find'
2069)    'ethod'
2070)    ' \r'
2071)    'ines'
2072)    ' inv'
2073)    ' point'
2074)    ' They'
2075)    ' used'
2076)    'ctions'
2077)    ' still'
2078)    'i√≥'
2079)    'ined'
2080)    ' while'
2081)    'It'
2082)    'ember'
2083)    ' say'
2084)    ' help'
2085)    ' cre'
2086)    ' x'
2087)    ' Tr'
2088)    'ument'
2089)    ' sk'
2090)    'ought'
2091)    'ually'
2092)    'message'
2093)    ' Con'
2094)    ' mon'
2095)    'ared'
2096)    'work'
2097)    '):'
2098)    'ister'
2099)    'arn'
2100)    'ized'
```

---

<br><br>
</details>

<details>
<summary>Unload model (button)</summary>

 ---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>PDF pages (button)</summary>

 ---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>PDF full (button)</summary>

 ---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>TXT system prompt (button)</summary>

---

Fills _System prompt_ field with a prompt saved in a TXT file.

Click the button to select the TXT file.

---

<br><br>
</details>

<details>
<summary>TXT user prompt (button)</summary>

---

Fills _User prompt_ field with a list of prompt saved in a TXT file.

Click the button to select the TXT file.


---

<br><br>
</details>

<details>
<summary>Copy HF links (button)</summary>

---

Copy a Hugging Face download model URL (Files and versions tab) and extract all links to `.gguf` files.

You can paste all the copied links into the _Dowonload models for testing_ field at once.

---

<br><br>
</details>

<details>
<summary>TXT Load URLs (button)</summary>

---

Fills _Download model for testing_ field with a list of model URLs saved in a TXT file.

Click the button to select the TXT file.

---

<br><br>
</details>


<details>
<summary>DB Browser (button)</summary>

---

Opens [DB Browser](https://sqlitebrowser.org/) if its directory is into Samantha's directory.

To use DB Browser, [download](https://sqlitebrowser.org/dl/) the `.zip`(no installer) version and unpack it into Samantha's directory: `samantha-ia-main\DB Browser for SQLite`.

---

<br><br>
</details>

<details>
<summary>D-Tale (button)</summary>

---

Opens [D-Tale](https://github.com/man-group/dtale) library interface in a new browser tab with a example dataset (titanic.csv).
<br><br>

**Web Client for Visualizing Pandas Objects**

>D-Tale is the combination of a Flask back-end and a React front-end to bring you an easy way to view & analyze Pandas data structures. It integrates seamlessly with ipython notebooks & python/ipython terminals. Currently this tool supports such Pandas objects as DataFrame, Series, MultiIndex, DatetimeIndex & RangeIndex. [D-Tale Project](https://pypi.org/project/dtale/#description)

<br>

A Windows terminal will also open.

---

<br><br>
</details>

<!--<details>
<summary>(button)</summary>
<br>

Teste

<br><br>
</details>-->

<br><br>
**Default Settings:**

<details>
<summary>Settings</summary>

---

Samantha's initial settings is **deterministic**. As a rule, this means that for the same prompt, you'll get always the same answer, even when applying penalties to exclude repeated tokens (penalties does not affect the model deterministic behavior).<br> 

<br>

üìê **Deterministic settings (default):** <br>
* reset_model (True)
* temperature (0)
* tfs_z (0)
* top_p (0)
* min_p (1)
* typical_p (0)
* top_k (40)
* presence_penalty (0)
* frequency_penalty (0)
* repeat_penalty (1)
<br><br>

Deterministic settings is used to assess training database biases.

Some models tend to loop (repeat the same text indefinitely) when using highly deterministic adjustments, selecting tokens with the highest probability score. 
Others may generate the first response with different content from subsequent ones. In this case, to always get the same response, activate the **_Reset model_** checkbox.

In turn, for **stochastic** behavior, suited for creative content, in which model selects tokens with different probability scores, adjust the hyperparameters accordingly.
<br><br>

üé® **Stochastic settings (example):** <br>
* reset_model (True)
* temperature (0.8)
* tfs_z (1)
* top_p (0.95)
* min_p (0.05)
* typical_p (1)
* top_k (40)
* presence_penalty (0)
* frequency_penalty (0)
* repeat_penalty (1.1)

<br>

You can use the _Learning Mode_ to monitor and adjust the degree of determinism/randomness of the responses generated by the model.

---

<br><br>
</details>

<br><br>

### Interface Right Column (output):

<details>
<summary>Assistant output (textbox)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Next token (button)</summary>

---

‚ûï Adds the next token to the model response when `Learning Mode` is enabled.

---

<br><br>
</details>

<details>
<summary>Copy Python code (button)</summary>

---

Copies Python code blocks generated by the model, present in the last response. The code must be enclosed in triple backticks:


\``` _python_**<br>
_(code)_

\```


Library installation code blocks starting with `pip` or `!pip` are ignored.

To manually run the Python code generated by the model, you must first copy it to the clipboard using this button.

---

<br><br>
</details>

<details>
<summary>Run Code (button)</summary>

---

This button executes any Python code copied to the clipboard that uses the libraries installed in the `jupterlab` virtual environment. Just select the code (even outside of Samantha), press `CTRL + C` and click the button.

Use it in combination with _Copy Python Code_ button.

The `pip` and `!pip` instructions lines present in the code are ignored.

Whenever Python code returns a value other than `''` (empty string), an HTML pop-up window opens to display the returned content.


---

<br><br>
</details>

<details>
<summary>Copy last response (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Copy all responses (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Open Jupyterlab (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Response in HTML (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Responses in HTML (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Voice control (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Audio player (widget)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Text to speech (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Last response (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>All responses (button)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Models repositories (links)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>Operating tips (tips)</summary>

---

üöß Under construction.

---

<br><br>
</details>

<details>
<summary>User prompt examples (prompts)</summary>

---

A collection of user prompts.

Just click to select a prompt and send it to the _User prompt_ field.

You can change this list of prompts by editing the `user_prompts.txt` file. Samantha must be restarted to display the changes.

---

<br><br>
</details>

<details>
<summary>System prompt examples (prompts)</summary>

---

A collection of system prompts.

Just click to select a prompt and send it to the _System prompt_ field.

You can change this list of prompts by editing the `system_prompts.txt` file. Samantha must be restarted to display the changes.

---

<br><br>
</details>


<br><br>
## ‚úè Prompt Engineering with Samantha

<details>

<summary>General instructions</summary>

---

**Prompt engineering guides:**

* [Prompt engineering guide](https://www.promptingguide.ai/pt)<br>
* [OpenAI prompt engineering](https://platform.openai.com/docs/guides/prompt-engineering)<br>
* [Anthropic prompt engineering](https://docs.anthropic.com/en/docs/build-with-claude/prompt-engineering/overview#prompting-vs-finetuning)<br>
* [Google prompt engineering](https://ai.google.dev/gemini-api/docs/prompting-intro)
<br><br>


ü¶ã **Butterfly effect:** _Small changes in the input text can lead to substantial changes in the output generated by the model._

Whatever you don't specify, if it required to generate the response, the model will decide for you.
<br>

---

<br><br>
</details>


<details>

<summary>Prompt chaining with single model</summary>

---

User prompt:

```
Translate to English and refine the following instruction:
"Crie um prompt para uma IA gerar um c√≥digo em Python que exiba um gr√°fico de barras usando dados aleat√≥rios contextualizados."
DO NOT EXECUTE THE CODE!
$$$

Refine even more the prompt in your previous response.
DO NOT EXECUTE THE CODE!
$$$

Execute the prompt in your previous response.
$$$

Correct the errors in your previous response, if any.
$$$
```

Settings:<br>
* Model: https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q4_K_M.gguf?download=true (you can just paste the URL in _Download model for testing_ field)
* Feedback loop: activated
* Run code automatically: activated

This prompt translate the initial instruction from Portuguese to English (instructions in English use to generate more accurate responses), transfers the task of refining the user's initial prompt to the model (models add detailed instructions), generating a more elaborate prompt. 

Each prompt is executed automatically and the model's response, as well as the output of the Python interpreter (if existing), are fed back to the model to generate the next response.

To create a prompt list like this, add one prompt at a time and test it. If the code runs correctly, add the next prompt considering the output of the previous one. Since you are using deterministic settings, the model output will be the same for the same input text.

Experiment with other models to test their behaviors. Change the initial prompt slightly to test the model's adherence.

---

</details>

<details>

<summary>Stop condition with single model</summary>

---

User prompt:
```
Follow the instructions below step by step:
1) Create a Python function that generates a random number between 1 and 10. If the number returned is less than 4, print that number. Otherwise, print ''.
2) Execute the function.
Attention: Write only the code. Do not include comments.
$$$
```

Settings:<br>
* Model: https://huggingface.co/chatpdflocal/llama3.1-8b-gguf/resolve/main/ggml-model-Q4_K_M.gguf?download=true (you can just paste the URL in _Download model for testing_ field)
* Number of reponses: 100
* Run code automatically: activated
* Stop condition: activated

This prompt creates and executes Python code sequentially until a condition is met (random number < 4), stopping Samantha.

You can ask the model to create any Python code and specify any condition to stop Samantha.<br>

---

</details>

<details>

<summary>Prompt engeneering with single model</summary>

---

User prompt:
```
Create a complete and detailed description of a prompt engineer specialized AI system.
Example: "I am an AI specialized in prompt engineering. My mission is to analyze the prompt provided by the user, identify areas that can be improved and generate an improved prompt...".
Finish by asking the user for a prompt to improve.
$$$

Based on your previous response, improve the following prompt:
PROMPT: "Create a bar chart using Python."
$$$

Generate the code described in your previous response.
$$$
```

Settings:<br>
* Model: https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q4_K_M.gguf?download=true (you can just paste the URL in _Download model for testing_ field)
* Number of reponses: 1
* Feedback loop: activated
* Run code automatically: activated

This prompt creates the persona of an AI prompt engineer, who refines the user's initial prompt and generates Python code. Finally, Samantha runs the code and displays the bar chart in a pop-up window.

You can submit any initial prompt to the model.

---

</details>
<br><br>

## ‚ñ∂Ô∏è Video Tutorials

<details>
<summary>Watch the videos</summary>

<br><br>
You can add text within a collapsed section. 
<br><br>
  
```python
   print("Hello World")
```
<br>

[![Watch the video](https://i.sstatic.net/Vp2cE.png)](https://youtu.be/vt5fpE0bzSY)

</details>

<br><br>
## ü™Ñ Samantha's Tips & Tricks

<details>
<summary>Tips & Tricks</summary>

<br><br>
* Keep only one interface window open at a time. Multiple open windows prevent the buttons from working properly. If there is more than one Samantha tab open, only one must be executed at a time (server side and browser side are independent).
* It is not possible to interrupt the program during the model loading and thinking phases, except by closing it. Pressing the stop buttons during these phases will only take effect when the token generation phase begins.
* You can select the same model more than once, in any sequence you prefer. To do so, select additional models at the bottom of the dropdown list. When deleting a model, all of the same type will be excluded from the selection.
* To create a new line in a field, press `SHIFT + ENTER`. If only the `ENTER` key is pressed, anywhere in the Samantha interface, the loading/processing/generation phases will begin.
* The pop-up window generated by the matplotlib module must be closed for the program to continue running.
* Whenever you save a new model on your computer, you must click on the "Load Model" button and select the folder where it is located so that it appears in the model selection dropdown.
* Changes made to the fields and interface settings during the model download and loading, processing and token generation will only be made the next time the Start Chat button or the `ENTER` key is pressed.
* To follow the text generation on the screen, click inside the _Assistant output_ field and press the Page Down key until you see the end of the text being generated by the model.
* You can use Windows keyboard shortcuts inside fields: `CTRL + A` (select all text)`, CTRL + C` (copy text), `CTRL + V` (paste copied text)  and `CTRL + Z` (undo) etc.
* To reload the browser tab, press `F5` and _Clear history_ button. This procedure will reset all fields and settings of the interface. If there was a model loaded via URL, it will remain loaded and accessible in the _Select model_ dropdown list as `MODEL_FOR_TESTING` (no need to re-download).
* If you accidentally close Samantha's browser tab, open a new tab and type `localhost` to display the full local URL: `http://localhost:7860/?__theme=dark`. In this case, Samantha's server must be running.
* When generating code incrementally, divide the user prompt into parts separated by `$$$` that generate blocks of code that complement each other.

<br>

</details>


<br><br>
## üôè‚Äã Help Improve Samantha

<details>
<summary>Your suggestions are welcome!</summary>

---

üåê Samantha is being developed under the principles of [Open Science](https://www.unesco.org/en/open-science) (open methodology, open source, open data, open access, open peer review and open educational resources). Therefore, anyone can contribute to its improvement. 

Feel free to share your ideas with us!

---

</details>


<br><br>
## üìù Version History and Future Improvements

<details>
<summary>Versions</summary>

<br><br>
üóìÔ∏è **Code Versions:**

Version 0.1.0 (2024-08-05):
* Initial beta version.
<br>

üöÄ **Future improvements:**
<br>

* Add a scatterplot for displaying the semantic diversity of the topics covered in the model response.
* Add Retrieval-Augmented Generation (RAG) feature
* Try other lightweight offline Speech-to-Text libraries with better speech recognition accuracy.
* Refactor the code to make it more manteinable.
<br>

Suggestions are always welcome!
<br>

</details>
<br><br>

<!--https://github.com/matiassingers/awesome-readme?tab=readme-ov-file#Examples-->
<!--https://huggingface.co/spaces/SoulAbi/text-to-voice-->
