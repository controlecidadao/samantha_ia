
![v](https://img.shields.io/badge/version-0.1.0-blue) 
[![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://opensource.org/licenses/MIT)
![v](https://img.shields.io/badge/updated-August%2005,%20%202024-green)

![banner](https://github.com/controlecidadao/samantha_ia/blob/main/images/banner.png)



## Samantha Interface Assistant: Experimental Environment Designed to Democratize the Use of Open Source Large Language Models (LLM)

### ‚ú® Welcome to Samantha: An Interface Assistant for Open Source Artificial Intelligence

<br>

üíª Samantha is an interface assistant for open source artificial intelligence models developed under [Open Science](https://www.unesco.org/en/open-science) principles (open methodology, open source, open data, open access, open peer review and open educational resources) for use on common Windows computers (without GPU). The program runs the LLM locally, free of charge and unlimitedly, without the need for an internet connection, except to download [GGUF](https://huggingface.co/docs/hub/gguf) models or when required by the execution of the code created by the models (g.e. for data analysis). Its objective is to democratize knowledge about the use of AI and demonstrate that, using the appropriate technique, even small models are capable of producing responses similar to those of larger proprietary ones. Her mission is to help explore the boundaries of (realy) open AI models.

<br>

üïµÔ∏è‚Äç‚ôÄÔ∏è It is being developed to assist in the exercise of social control of the public administration, considering the worrisome current scenario of the growing loss of citizens' trust in external control institutions, resulting from the destructuring of the technical staff of the courts of accounts. However, its features allow it to be used by anyone interested in exploring open-source artificial intelligence models, especially programmers.

<br>

‚ôæÔ∏è The system allows the sequential loading of a list of prompts and models (one model at a time to save memory), as well as the adjustment of their hyperparameters, allowing the response generated by the previous model to be feedbacked and analyzed by the subsequent model to generate the next response, in an unlimited number of interaction cycles between LLMs without human intervention. Models can only interact with the answer provided by the immediately preceding model, so each new response replaces the previous one. You can also use just one model and have it interact with its previous response over an unlimited number of text generation cycles.
<br><br>

üîó Some chaining examples **_without using_** Samantha's response **Feedback Loop** feature:

  * **(model_1) responds (prompt_1) X number of responses:** used to analyze model's deterministic and stochastic behavior with help of the Learning Mode, as well as to generate multiple diverse responses with stochastic settings.

  * **(model_1) responds (prompt_1, prompt_2, prompt_n):** used to execute multiples instructions sequencially with the same model.

  * **(model_1, model_2, model_n) respond (prompt_1):** used to compare models' responses for the same single prompt. Useful for comparing different models, as well as quantized versions of the same model.

  * **(model_1, model_2, model_n) respond (prompt_1, prompt_2, prompt_n):** used to compare models' responses for a list of prompts, as well as to execute a sequence of instructions using disctinct models. By using the Single Response per Model feature, each model can also respond to only one specific prompt.
<br><br>

üîó Some chaining examples **_using_** Samantha's response **Feedback Loop** feature:

  * **(model_1) responds (prompt_1) X number of responses:** Used to improve model's previous response through a fixed user instruction using the same model, as well as to generate a continuous dialog using a single model.

  * **(model_1) responds (prompt_1, prompt_2, prompt_n):** used to improve model's previous response through multiples user instructions sequencially with the same model. Each prompt is used to refine the previous response.

  * **(model_1, model_2, model_n) respond (prompt_1):** Used to improve previous model's response using disctinct models, as well as to generate a dialog between different models.

  * **(model_1, model_2, model_n) respond (prompt_1, prompt_2, prompt_n):** Used to execute a sequence of instructions using disctinct models (Single Response per Model feature).
<br><br>


üëâ **Chaining Sequence Template:   ( [models list] -> respond -> ( [user prompt list] X number of responses) ) X number of loops**
<br><br><br>

<p align="center">
  <a href="https://youtu.be/vt5fpE0bzSY">
    <img src="https://i.sstatic.net/Vp2cE.png" alt="Watch the video">
  </a>
</p>

<br>

üß© Sequencing of prompts and models allows the generation of long responses by fractioning the user input instruction. Every partial response fits in the model's response length defined in the training process.
<br><br>

üîß As an open source tool for automatic self-interaction between AI models, Samantha Interface Assistant was designed to explore **reverse prompt engineering with self-improvement feedback loop** üîÅ. This technique helps small large language models (LLM) to generate more accurate responses by transferring to the model the task of creating the final prompt and corresponding response based on the user's initial imprecise instructions, adding intermediate layers to the prompt construction process. Samantha doesn't have a hidden system prompt like it does with proprietary models. All instructions are controlled by the user.
<br><br>

üé≤ Thanks to **emergent behavior**, with the right prompt and proper hyperparameter configuration, even small models working together can generate big responses!
<br><br>

üåé **A Small Step:** Samantha is just a movement towards a future where artificial intelligence is not a privilege but a tool for all in a world where individuals can leverage AI to enhance their productivity, creativity, and decision-making without barriers, walking a journey to democratize AI and make it a force for good in our daily lives.

<br>

‚ö†Ô∏è **Use Responsibly:**
The generated text reflects the content, biases, errors and improprieties present in their training datasets. We encourage responsible use of Samantha and for insights only, always keeping ethical considerations at the forefront of our interactions with AI algorithms, which are complex mathematical models that generates coherent texts from the sequencing of words (tokens) based on the probability patterns extracted from the training texts.

<br>

ü¶æ **The Instrumental Nature of AI:** Recognizing the technological monopoly of artificial intelligence as a possible instrument of domination and the expansion of social inequalities represents a challenge at this inflection point in history. Noting the flaws of the smaller models during the text generation process aids in this understanding by comparing them with the claimed perfection of the larger proprietary models. It is necessary to reposition things in their proper places and question the romantic reductionist view of attributing human characteristics - such as intelligence (Anthropomorphization caused by Pareidolia) - to a technology produced by the human intellect. For this reason, it is essential to demystify artificial intelligence through a didactic approach to how this novel "word calculator" works. Certainly, the dopamine of the initial charm artificially created by the market will not withstand the generation of a few hundred tokens (token is the name given to the basic building block of texts that an LLM uses to understand and generate text. A token may be an entire word or part of a word).

<br>

‚úèÔ∏è **Text Generation Considerations:**
Users should be aware that the responses generated by AI are derived from the training of its large language models on a vast corpus of text data. The exact sources or processes used by the AI to generate its outputs cannot be precisely cited or identified. The content produced by the AI is not a direct quotation or compilation from specific sources. Instead, it reflects the patterns, statistical relationships, and knowledge that the AI's neural networks have learned and encoded during the training process on the broad data corpus. The responses are generated based on this learned knowledge representation, rather than being retrieved verbatim from any particular source material. While the AI's training data may have included authoritative sources, its outputs are its own synthesized expressions of the learned associations and concepts.

<br>

üéØ **Objective:**
The primary objective with Samantha is to **inspire** üí° others to create similar - and much better ones, to be sure - systems and to educate users on the utilization of AI. Our goal is to foster a community of developers and enthusiasts who can take the knowledge and tools to further innovate and contribute to the field of open source AI. By doing so, the aim to cultivate a culture of collaboration and sharing, ensuring that the benefits of AI are accessible to all, regardless of their technical background or financial resources. It is believed that by enabling more people to construct and comprehend AI applications, we can collectively drive progress and address **societal challenges** with informed and diverse perspectives. Let's work together to shape a future where AI is a **positive and inclusive force for humanity**. See [UNESCO's Ethics of Artificial Intelligence Recommendations](https://www.unesco.org/en/artificial-intelligence/recommendation-ethics?hub=32618).

<br>

üö® **The Human Cost of Innovation:**
While this system aims to empower users and democratize access to AI, it's crucial to acknowledge the ethical implications of this technology. The development of powerful AI systems often relies on the exploitation of human labor, particularly in data annotation and training processes. This can perpetuate existing inequalities and create new forms of digital divide. **As users of AI, we have a responsibility to be aware of these issues and advocate for fairer practices within the industry**. By supporting ethical AI development and promoting transparency in data sourcing, we can contribute to a more inclusive and equitable future for all.

  * [Como funciona o trabalho humano por tr√°s da intelig√™ncia artificial](https://www.youtube.com/watch?v=F0M9OH5n-hg)

  * [The "Modern Day Slaves" Of The AI Tech World](https://www.youtube.com/watch?v=VPSZFUiElls)

<br>

üôè **On the Shoulders of Giants:**
Special thanks to Georgi Gerganov and the whole team working on [llama.cpp](https://github.com/ggerganov/llama.cpp) for making all of this possible, as well as to [Andrei Bleten](https://github.com/abetlen/llama-cpp-python) by his amazing Python bidings for the Gerganov C++ library.

<br><br>





## üìå Samantha's Key Features

<details>
<summary>Features</summary>

<br><br>
‚úÖ **Open Source Foundation:** Built upon [Llama.cpp](https://github.com/ggerganov/llama.cpp) / [llama-cpp-python](https://github.com/abetlen/llama-cpp-python) and [Gradio](https://www.gradio.app/) , under [MIT license](https://opensource.org/license/mit), Samantha runs on standard computers, even without a dedicated Graphics Processing Unit (GPU).<br><br>
  
‚úÖ **Offline Capability:** Samantha operates independently of the internet, requiring connectivity only for the initial download of model files or when required by the execution of the code created by the models. This ensures privacy and security for your data processing needs. Your sensitive data is not shared via the internet with companies through confidentiality agreements.<br><br>

‚úÖ **Unlimited and Free Use:** Samantha's open-source nature allows for unrestricted use without any costs or limitations, making it accessible to everyone in everywhere.<br><br>

<!-- **Accessibility for People with Disabilities**: The system is designed to be user-friendly and accessible for people with physical disabilities. With features like voice interaction through text-to-speech and speech-to-text, users can interact with Samantha without relying solely on visual interfaces. This inclusive design ensures that AI technology is available to everyone, regardless of their abilities.-->

‚úÖ **Extensive Model Selection:** With access to [thousands](https://huggingface.co/models?sort=trending&search=gguf) of foundation and fine-tuned open-source models, users can experiment with various AI capabilities, each tailored to different tasks and applications, allowing to chain the sequence of models that best meet your needs.<br><br>

‚úÖ **Copy and paste LLMs:** To try out a sequence of `gguf` models, just copy their download links from any Hugging Face repository and paste inside Samantha to run them right away in sequence.<br><br>

‚úÖ **Customizable Parameters:** Users have control over model hyperparameters such as **context window** length (n_ctx, max_tokens), **token sampling** (temperature, tfs_z, top-k, top-p, min_p, typical_p), **penalties** (presence_penalty, frequency_penalty, repeat_penalty) and **stop words**, allowing for responses that suit specific requirements, with deterministic or stochastic behavior.<br><br>

‚úÖ **Interactive Experience:** Samantha's chaining functionality enables users to generate endless texts by chaining prompts and models, facilitating complex interactions between different LLMs without human intervention.<br><br>

‚úÖ **Feedback Loop:** This feature allows you to capture the response generated by the model and feed it back into the next cycle of the conversation.<br><br>

‚úÖ **Learning Insights:** A feature called 'Learning Mode' lets users observe the model's decision-making process, providing insights into how it selects output tokens based on their probability scores (logits) and hyperparameter settings. A list of the least likely selected tokens is also generated.<br><br>

‚úÖ **Voice Interaction:** Samantha supports simple voice commands with offline speech-to-text [Vosk](https://alphacephei.com/vosk/) (English and Portuguese) and text-to-speech with SAPI5 voices, making it accessible and user-friendly.<br><br>

‚úÖ **Audio feedback:** The interface provides audible alerts to the user, signaling the beginning and end of the text generation phase by the model.<br><br>

‚úÖ **Document Handling:** The system can load small PDF and TXT files. Chaining instructions/prompts can be inputted via a TXT file for convenience.<br><br>

‚úÖ **Versatile Text Input:** Fields for prompt insertion allow users to interact with the system effectively, including system prompt, previous model response and user prompt to guide the model's response.<br><br>

‚úÖ **Code Integration:** Automatic extraction of code blocks from model's response, along with pre-installed [JupyterLab](https://jupyter.org/) in an isolated virtual environment, enables users to execute generated code swiftly for immediate results.<br><br>

‚úÖ **Run Code Button:** The system allows the user to edit and run the code generated by the model simply by clicking the ‚ÄúRun Code‚Äù button. The output will be displayed in an HTML pop-up window;<br><br>

‚úÖ **Automatic Code Execution:** Samantha features the option to automatically run the Python code generated by the models sequentially.<br><br>

‚úÖ **"#IDE" feature:** Users can select and run any Python code that uses the libraries installed in the `jupyterlab` virtual environment by entering the `#IDE` comment in the code (even for code outside of Samantha), enclosing it with ` ```python ` (Python code here) ` ``` `, copying it and clicking the "Run Code" button;<br><br>

‚úÖ **Incremental Coding:** Using deterministic settings, create Python code incrementally, making sure each part works before moving on to the next.<br><br>

‚úÖ **Complete access and control:** Through the ecosystem of Python libraries and the codes generated by the models, it is possible to access computer files, allowing you to read, create, change and delete local files, as well as access the internet, if available, to upload and download information and files.<br><br>

‚úÖ **Data Analysis Tools:** A suite of data analysis tools like [Pandas](https://pandas.pydata.org/), [Numpy](https://numpy.org/), [SciPy](https://scipy.org/), [Scikit-Learn](https://scikit-learn.org/stable/index.html#), [Seaborn](https://seaborn.pydata.org/), [Vega-Altair](https://altair-viz.github.io/), [Plotly](https://plotly.com/python/), [Bokeh](https://docs.bokeh.org/en/latest/index.html), [Dash](https://plotly.com/examples/), [Sweetviz](https://pypi.org/project/sweetviz/), [D-Tale](https://github.com/man-group/dtale), [DataPrep](https://dataprep.ai/), [NetworkX](https://networkx.org/), [Pyvis](https://pyvis.readthedocs.io/en/latest/index.html), [Selenium](https://selenium-python.readthedocs.io/), [PyMuPDF](https://pypi.org/project/PyMuPDF/) and [Beautiful Soup](https://beautiful-soup-4.readthedocs.io/en/latest/) are available within JupyterLab for comprehensive analysis and visualization (for a complete list of all Python available libraries, use a prompt like **_"create a Python code that prints all modules installed. Separate each module with \<br\> tag."_** and press Run Code button. The result will be displayed in a browser popup). Integration with [DB Browser](https://sqlitebrowser.org/about/) is also available.<br><br>

‚úÖ **Performance Optimized:** To ensure smooth performance on CPUs, Samantha maintains a limited chat history to just the previous response, reducing the model's context window size to save memory and computational resources.<br>

</details>





<br><br>
## üõ†Ô∏è Installing Samantha

<details>
<summary>Instructions</summary>

<br><br>
To use Samantha you will need:
<br><br>
* Install [Visual Studio](https://visualstudio.microsoft.com/pt-br/vs/community/) (free community version) on your computer. Download it, run it, and select only the option **Desktop development with C++** (administrator privileges required):

  ![cmake](https://github.com/controlecidadao/samantha_ia/blob/main/images/cmake2.png)
<br><br>
* Download the zip file from Samantha's repository by clicking [here](https://github.com/controlecidadao/samantha_ia/archive/refs/heads/main.zip) and unzip it to your computer. Select the drive where you want to install the program:

   ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/directory.png)
<br><br>
* Open `samantha_ia-main` directory and double click on `install_samantha_ia.bat` file to start installation. Windows may ask you to confirm the origin of the `.bat` file. Click on 'More info' and confirm. We encorage to inspect the code of all files:

   ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/install.png)<br><br>

  >_This is the critical part of the installation. If everything goes well, the process will complete without displaying error messages in the terminal._<br>

  <br>
  
  The installation process takes about _**50 minutes**_ and should end with the creation of two virtual environments: `samantha` and `jupyterlab`. It will take up about _**5 GB**_ of your hard drive.

<br>

* Once installed, open Samantha by double clicking on `open_samantha.bat` file. Windows may ask you again to confirm the source of the `.bat` file. This authorisation is required only the first time you run the program. Click on 'More info' and confirm:<br>

  ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/open_samantha.png)<br><br>

  A terminal window will open. This is the Samantha's **server-side**.

  After answering the initial questions (interface language and voice control options - voice control is not suitable for first use), the interface will open in a new browser tab. This is the Samantha's **browser-side**:

  <br>

  ![directory](https://github.com/controlecidadao/samantha_ia/blob/main/images/interface_english.png)<br><br>

  With the browser window opened, Samantha is ready to go.

</details>





<br><br>
## üëü Testing a Model in 5 Steps

<details>
<summary>Instructions</summary>

<br><br>
Samantha needs just a `.gguf` model file to generate text. Follow these steps to perform a simple model test with Samantha:
<br><br>

1) Open Microsoft Task Management by pressing `CTRL + SHIFT + ESC` and check available memory. Close some programs if necessary to free memory.

2) Visit [Hugging Face](https://huggingface.co/models?library=gguf&sort=trending&search=gguf) repository and click on the card to open the corresponding page. Locate the _Files and versions_ tab and choose a `gguf` model that fits in your available memory.
   
4) Right click over the model download link icon and copy its url.

5) Paste the model url into Samantha's _Download Models for Testing_ field.

6) Insert a prompt into _USER Prompt_ field and press `Enter` (keep the $$$ sign at the end of your prompt). The model will be downloaded and the response will be generated using a deterministic settings. You can track this process via Task Management.

<br>

Every new model downloaded via this copy and paste procedure will replace the previous one to save hard drive space.

You can also download the model and save it permanently to your computer. For more datails, see the section below.

</details>





<br><br>
## ‚¨áÔ∏è Downloading Large Language Models (LLM)

<details>
<summary>Instructions</summary>

<br>

### Downloading Open Source Model Files (.gguf)

Open souce model can be downloaded from [Hugging Face](https://huggingface.co/models?sort=trending&search=gguf), using `gguf` as the search parameter. You can combine two words like `gguf code`.

You can also go to a specific repository and see all the `.gguf` models available for downloading and testing, like [https://huggingface.co/bartowski](https://huggingface.co/bartowski)
<br><br>

The models are displayed on cards like this:

![model_card](https://github.com/controlecidadao/samantha_ia/blob/main/images/model_card.png)
<br><br>

To download the model, click on the card to open the corresponding page. Locate the **Model card** and **Files and versions** tabs:

![tabs](https://github.com/controlecidadao/samantha_ia/blob/main/images/tabs.png)
<br><br>

After that, click on the **Files and versions** tab and download a model that fits in your available RAM space. To check your available memory, open Task Manager by pressing `CTRL + SHIFT + ESC`, click on **Performance** tab (1) and select **Memory** (2):

<br>

![task](https://github.com/controlecidadao/samantha_ia/blob/main/images/task_manager.png)
<br><br>

We suggest to download the model with **Q4_K_M** (4-bit quantization) in its link name (put the mouse over the download button to view the complete file name in the link like this: `https://huggingface.co/NousResearch/Hermes-2-Pro-Llama-3-8B-GGUF/resolve/main/Hermes-2-Pro-Llama-3-8B-Q4_K_M.gguf?download=true`). As a rule, the larger the model size, the greater the accuracy of the generated text.
<br>

If the downloaded model doesn't fit into the available RAM space, your hard drive will be used, impacting performance.

Download the chosen model and save it to your computer or just copy the download link and paste it inside Samantha's _Download model for testing_ field. Watch video tutorials in the section below for more details.
<br><br>

Note that each model has its own characteristics, presenting significantly different responses depending on its size, internal architecture, training method, predominant language of the training database, user prompt and hyperparameter adjustment, and it is necessary to test its performance for the desired task.

**Some models may not be loaded due to their technical characteristics or incompatibility with the current version of the [llama.cpp Python binding](https://github.com/abetlen/llama-cpp-python) used by Samantha**.

Where to find models to test: [Huggingface GGUF Models](https://huggingface.co/models?sort=trending&search=gguf)<br><br>

The quality of the response generated by a model can be evaluated using some basic criteria, such as:

  * **Degree of understanding** of the explicit and implicit instructions contained in the user prompt;

  * **Degree of obedience** to these instructions;

  * **Degree of precision in the decision-making process** to fill in the gaps in the context of the user prompt and to resolve ambiguities necessary to generate the response;

  * **Degree of coherence of the bias adopted by the model** with the bias (or lack thereof) contained in the user's prompt;

  * **Degree of pertinence and relevance of the topics choosen** to be addressed;

  * **Degree of breadth and depth of approach to topics** in the response;

  * **Degree of sintatic and semantic precision** of the response;

  * **Quality of the structure and content of the response** in relation to the user's expectations (and their overcoming) for the problem submitted to the model, considering the technique used to create the prompt (prompt engineering) and the adjustment of the model's hyperparameters.
</details>





<br><br>
## üß† Samantha's Controls & Settings - üöß Under Construction

### Interface Left Column (input):
<br>

**Main Controls:**
<details>
<summary>Start chat (button)</summary>
 
---
 
Starts a chat session, sending all input texts (system prompt, assistant previous response and user prompt) to the server, as well as the settings adjusted by the user. Just like all other buttons, a mouse click will sound.

Keyboard shortcut: Press "Enter" anywhere on the page.

A model must be pre-selected in _Model selection_ field or a URL must be provided to _Download model for testing_ field.

---

<br><br>
</details>

<details>
<summary>Stop / Next (button)</summary>

---

Interrupts the token generation process for the current model, starting the execution of the next model in the sequence, if any.

It also stops playback of the currently playing audio when in speech autoplay mode (_Read response aloud_ checkbox selected).

Samantha has 3 phases: loading model (non stop), thinking (non stop) and next token selection (stop). This button works only when the next token selection phase is started, even if it was pressed previously.

---

<br><br>
</details>

<details>
<summary>Clear history (button)</summary>

---

Clears the history of the current chat session, erasing the assistant output field as well as all internal logs, previous response etc.

---

<br><br>
</details>

<details>
<summary>Load model (button)</summary>

---

Allows you to select the directory where the models available for loading are saved.

Default: Windows "Downloads" folder

You can select any directory that contains GGUF models. In this case, the models contained in the selected directory will be listed in the _Model selection_ dropdown list.

When the pop-up window opens, make sure to click on the folder you want to select.

---

<br><br>
</details>

<details>
<summary>Stop all & reset (button)</summary>

---

Stops the sequence of running models and resets internal settings of the last loaded model.

After resetting, models take some time to restart text generation, depending on the size of the input text.

---

<br><br>
</details>

<details>
<summary>Replace response (button)</summary>

---

Replaces the text in the _Assistant previous response_ field with the text of the last response generated by the model. 

The replaced text will be used as the model's previous response in the next conversation cycle. 

This replaced text is not visible. It does not erase text from the _Previous Assistant Response_ field, which can be used again later.

---

<br><br>
</details>


<details>
<summary>System prompt (textbox)</summary>

---

In the context of Large Language Models (LLMs), a system prompt is a special type of instruction given to the model at the beginning of a conversation or task. It is considered in all interactions with the model.

Think of it as setting the stage for the interaction. It provides the LLM with crucial information about its role, the desired persona, behavior, and the overall context of the conversation.

Here's how it works:

1. Defining the Role: The system prompt clearly defines the LLM's role in the interaction. 
    * For example, it might instruct the model to act as a helpful assistant, a creative writer, a factual summarizer, or even a character in a story.

2. Setting the Tone and Persona:  The system prompt can also establish the desired tone and persona for the LLM's responses. 
    * It could be formal, informal, humorous, serious, or any other style depending on the intended use case.

3. Providing Contextual Information: The system prompt can offer background information relevant to the conversation or task. 
    * This helps the LLM understand the user's needs and provide more accurate and relevant responses.

Benefits of Using System Prompts:

* Improved Consistency: System prompts ensure that the LLM consistently adheres to a specific role and style throughout the interaction.
* Enhanced Accuracy: By providing context and instructions, system prompts help the LLM generate more accurate and relevant responses.
* Tailored Experiences: Different system prompts can be used to create tailored experiences for users based on their needs and preferences.

Example:

Let's say you want to use an LLM to write a poem in the style of Shakespeare. A suitable system prompt would be:

"You are William Shakespeare, a renowned poet from Elizabethan England. Write a sonnet about the beauty of a summer day."

By providing this system prompt, you guide the LLM to generate a response that reflects Shakespeare's language, style, and thematic interests.

Not all models support system prompt. Test to find out. Fill in "x = 2" in the system prompt field and ask the model the value of "x" in the user prompt field. If the model gets the value of "x", system prompt is available in the model.

You can simulate the effect of the system prompt by adding text in square brackets in the user prompt field: [This text acts as a system prompt]

---

<br><br>
</details>

<details>
<summary>Feedback loop (checkbox)</summary>

---

When activated, it automatically considers the response generated by the model in the current conversation cycle as being the model's previous response in the next cycle, allowing feedback from the system. 

Any text entered by the user in the _Assistant previous response_ field is only considered in the first cycle after activating this feature. In the following cycles, the model's response internally replaces the previous response, but without deleting the text contained in that field, which can be reused in a new chat session. You can monitor the content of the assistant previous response via terminal.

In turn, when deactivated, it always uses the text contained in the _Assistant previous response_ field as the previous response, unless the text is preceded by `---` (triple dash). Text preceded by `---` is ignored by the model.

To internally clear the model's previous response, press the _Clear history_ button. It is recommended to press the _Clear history_ button before each new conversation when Feedback Loop feature is activated.

---

<br><br>
</details>

<details>
<summary>Assistant previous response (textbox)</summary>

---

Stores the text considered by the model as its previous response in the current conversation cycle.

Used to feed back the responses generated by the model.

To ignore the text present in this field, include `---` at the beginning.

---

<br><br>
</details>

<details>
<summary>User prompt (textbox)</summary>

---

The main input field of the interface. It receives the list of user prompts that will be submitted to the model sequentially.

Each item in the list must be separated from the next one by a line break (SHIFT + ENTER or "\n") or by the symbols `$$$` (triple dollar signal), if the items are made up of text with line breaks.

You can import a TXT file containing a list of prompts.

`---` before a prompt list item causes the system to ignore that item.

Text positioned within single square brackets (`[` and `]`) is added to the beginning of each prompt list item, simulating a system prompt.

Text positioned within double square brackets (`[[` and `]]`) is added as the last item in the prompt list. In this case, all responses generated by the model in the current chat session are concatenated and added to the end of this item, allowing the model to analyze them together.

If the model generates the word `STOP_SAMANTHA`, it stops token generation and exits the loop.

Example:<br>

_[You are a poet that writes only in Portuguese]_<br>
_Create a sentence about love_<br>
_Create a sentence about life_<br>
---_Create a sentence about time (this instruction is ignored)_<br>
_[[Create a paragraph in English that summarizes the ideas contained in the following sentences:]]_<br>
(_previous responses are concatenated here_)<br>

Model responses sequence:<br>

_"O amor √© um fogo que arde no meu peito, uma chama que me guia atrav√©s da vida."_<br>
_"A vida √© um rio que flui sem parar, levando-nos para al√©m do que conhecemos."_<br>
_Love and life are intertwined forces that shape our existence. Love burns within us like a fire, guiding us through life's journey with passion and purpose. Meanwhile, life itself is a dynamic and ever-changing river, constantly flowing and carrying us beyond the familiar and into the unknown. Together, love and life create a powerful current that propels us forward, urging us to explore, discover, and grow._

---

<br><br>
</details>

<details>
<summary>Model selection (dropdown)</summary>

---

Dropdown list of models saved on the computer and available for text generation.

To view models in this field, click the _Load model_ button and select the folder containing the models.

The default location for saving models is the Windows "Downloads" directory.

You can select multiples models (even repeated) to create a sequence of models to respond the user prompts.

The last model downloaded from a URL is saved as _MODEL_FOR_TESTING.gguf_ and is also displayed in this list.

---

<br><br>
</details>

<details>
<summary>Download model for testing (textbox)</summary>

---

Receives a list of Hugging Face links to the models that will be downloaded and executed sequencially.

Example: 

* https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q4_K_M.gguf?download=true

Links preceded by `---` will be ignored.

Only works if no model is selected in _Model selection_ dropdown list.

---

<br><br>
</details>

<details>
<summary>Single response per model (checkbox)</summary>

---

Activates a single response per model. 

Prompts that exceed the number of models are ignored.

Models that exceed the number of prompts are also ignored. 

This checkbox disables _Number of loops_ and _Number of responses_ checkboxes.

---

<br><br>
</details>

<details>
<summary>Reset model (checkbox)</summary>

---

Reinitializes the internal state of the model, eliminating the influence of the previous context.

How it Works:

When the reset feature is invoked:

* The model's internal state related to the current context is cleared.
* Any accumulated tokens from previous interactions are discarded.
* The model essentially returns to its initial state, as if it was just loaded.

Benefits:

* Improved consistency: Each new interaction starts fresh, reducing the chance of the model being influenced by unrelated previous context.
* Better control: Users can manage when the model should "forget" previous interactions.
* Performance optimization: By clearing unnecessary context, the model can operate more efficiently.


Use Cases:

* In chatbots or conversational AI where you want to start new sessions cleanly.
* In applications processing multiple independent text generation tasks.
* When fine-tuning or testing the model's behavior under controlled conditions.

---

<br><br>
</details>

<details>
<summary>Shuffle models (checkbox)</summary>

---

Shuffles the execution order of the models if 3 or more models are selected in dropdown list.

---

<br><br>
</details>

<details>
<summary>Fast mode (checkbox)</summary>

---

Generates text faster in the background without displaying the addition of each token in the assistant output field.

Minimizing or hiding the Samantha browser window makes the token generation process even faster.

This checkbox disables Learning Mode.

---

<br><br>
</details>

<details>
<summary>Voice selection (dropdown)</summary>

---

Selects the language of the computer's SAPI5 voice that will read the responses generated by the model.

---

<br><br>
</details>

<details>
<summary>Read response aloud (checkbox)</summary>

---

Activates automatic reading mode for responses generated by the model using the language selected in the _Voice selection_ dropdown list.

---

<br><br>
</details>

<details>
<summary>Learning mode (radio buttons)</summary>

---

Activates Learning Mode.

It presents a series of features that help in understanding the token selection process by the model, such as:

* Model metadata
* Tokens vocabulary
* Top-k tokens sorted by logits score with vocabulary position and selected token indication
* Barplot of the top-k tokens sorted by logits scores
* Cummulative barplot of the selected unlikely tokens

Only works if Fast Mode is unchecked. 

Radio buttons options:<br>

* OFF: Learning mode disabled.<br>
* 0, 0.3, 1, 3, 10: Time delay in seconds.<br>
* NEXT TOKEN: Allows you to control the response generation process, token by token, via _NEXT TOKEN_ button.<br>

---

<br><br>
</details>

<details>
<summary>Number of loops (radio buttons)</summary>

---

Set the number of repetitions of the selected model sequence in the following template:

Chaining Sequence Template: ( [models list] -> respond -> ( [user prompt list] X number of responses) ) X **_number of loops_**

Explanation:

Each model in the _models list_ responds to all prompts in the _user prompt list_ for the selected _number of responses_. This block is repeated for the selected _number of loops_.

---

<br><br>
</details>

<details>
<summary>Number of responses (radio buttons)</summary>

---

Number of responses to be generated by each selected model in the following template:

Chaining Sequence Template: ( [models list] -> respond -> ( [user prompt list] X **_number of responses_**) ) X _number of loops_

Explanation:

Each model in the _models list_ responds to all prompts in the _user prompt list_ for the selected _number of responses_. This block is repeated for the selected _number of loops_.


---

<br><br>
</details>

<details>
<summary>Run code automatically (checkbox)</summary>

---

When checked, runs automatically the Python code generated by the model.

---

<br><br>
</details>

<br><br>
**Context Window:**

<details>
<summary>n_ctx (slider)</summary>

---

`n_ctx` stands for _number of context tokens_ (context window) and determines the maximum number of tokens that the model can process at once. It determines how much previous text the model can "remember" and utilize when selecting the next token from model vocabulary.

The context length directly impacts the memory usage and computational load. Longer `n_ctx` requires more memory and computational power.

How `n_ctx` works:

It sets the upper limit on the number of tokens the model can "see" at once.
Tokens are usually word parts, full words, or characters, depending on the tokenization method.
The model uses this context to understand and generate text.
For example, if `n_ctx` is 2048, the model can process up to 2048 tokens at a time.

Impact on model operation:

During training and inference, the model attends to all tokens within this context window.<br>
It allows the model to capture long-range dependencies in the text.<br>
Larger `n_ctx` enables the model to handle longer sequences of text without losing earlier context.<br>

Why increasing `n_ctx` increases memory usage:

Attention mechanism: LLMs use self-attention mechanisms (like in Transformers) which compute attention scores between all pairs of tokens in the input.<br>
Quadratic scaling: The memory required for attention computations scales quadratically with the context length. If you double `n_ctx`, you quadruple the memory needed for attention.<br>

**CAUTION: `n_ctx` > `max_tokens` + number of input tokens** (system prompt + assistant previous response + user prompt).

When set to `0`, the system will use the maximum `n_ctx` possible (model's context window size).<br>
As a rule, set `n_ctx` equals to `max_tokens`, but both as higher as possible, so that they can't be reached. Samantha's default values for `n_ctx` and `max_tokens` are 4000 tokens.<br>
Before adjusting `n_ctx`, you must to unload the model by clicking _Unload model_ button.

Example:

User prompt = 6 tokens<br>
`n_ctx`= 20 tokens<br>
If the text generated by the model is equals or greater than 14 tokens (20 - 6), the system will raise an `IndexError` in the terminal, but the interface will not crash.

To check the impact of the `n_ctx` in memory, open Windows Task Manager (CTRL + SHIFT + ESC) to monitor memory usage, select memory panel and vary `n_ctx` values. Don't forget to unload model between changes.

---

<br><br>
</details>

<details>
<summary>max_tokens (slider)</summary>

---

Controls maximum number of tokens to be generated by the model.

Select `0` for the models' maximum number of tokens.

How `max_tokens` Works:

1. Sampling Process: When generating text, LLMs predict the next token based on the context provided. This prediction involves calculating probabilities for each possible token in the vocabulary.
   
2. Token Limit: The `max_tokens` parameter sets a hard limit on how many tokens the model can generate before stopping, regardless of the predicted probabilities.

3. Truncation: Once the generated text reaches `max_tokens`, the generation process is abruptly terminated. This means the final output might be incomplete or feel cut off.

---

<br><br>

</details>

<br><br>
**Stop Words:**

<details>
<summary>stop (textbox)</summary>

---

List of characters that interrupts text generation, in the format `["$$$", ".", ".\n"]` (Python list).

---

<br><br>
</details>

<br><br>
**Token Sampling:**

<details>
<summary>temperature (slider)</summary>
<br>

Temperature is a hyperparameter that controls the randomness of the text generation process in LLMs. It affects the probability distribution of the model's next-token predictions.

**Controlling Creativity:**

Use higher temperatures when you want the model to generate more creative, unexpected, and varied responses. This is useful for creative writing, brainstorming, and exploring multiple ideas.

Use lower temperatures when you need more predictable and focused output. This is useful for tasks requiring precise and reliable information, such as summarization or answering factual questions.

**High Temperature (T > 1):**

When T > 1, the logits are divided by a number greater than 1, making the differences between logits smaller.
This flattens the probability distribution, making the model more likely to sample less probable tokens.
The generated text becomes more diverse and creative, but potentially less coherent.

**Low Temperature (T < 1):**

When T<1, the logits are divided by a number less than 1, making the differences between logits larger.
This sharpens the probability distribution, making the model more likely to sample the most probable tokens.
The generated text becomes more focused and deterministic, but potentially less creative.

**Temperature = 1:**

When T = 1, the logits remain unchanged, and the model samples tokens based on the original probability distribution.
This is a balanced setting, maintaining a mix of coherence and diversity.

**Avoiding Repetition:**

Higher temperatures can help reduce repetitive patterns in the generated text by promoting diversity.

Very low temperatures can sometimes lead to repetitive and deterministic outputs, as the model might keep choosing the highest-probability tokens.

<br><br>
</details>

<details>
<summary>tfs_z (slider)</summary>
<br>

`tfs_z` stands for **Tail-free sampling with z-score**. It's a hyperparameter used in a text generation technique designed to balance the trade-off between diversity and quality in generated text.

**Context and purpose:**

Tail-free sampling was introduced as an alternative to other sampling methods like `top-k` or nucleus (`top-p`) sampling. Its goal is to remove the arbitrary "tail" of the probability distribution while maintaining a dynamic threshold.

**Technical Details of tfs_z in LLM Text Generation**

**Probability distribution analysis:**

The method examines the probability distribution of the next token predictions.
It focuses on the "tail" of this distribution - the less likely tokens.

**Z-score calculation:**

For each token in the sorted (descending) probability distribution, a z-score is calculated.
The z-score represents how many standard deviations a token's probability is from the mean.

**Cutoff determination:**

The `tfs_z` parameter sets the z-score threshold.
Tokens with a z-score below this threshold are removed from consideration.

**Dynamic thresholding:**

Unlike fixed methods like `top-k`, the number of tokens retained can vary based on the shape of the distribution.
This allows for more flexibility in different contexts.

**Sampling process:**

After applying the `tfs_z` cutoff, sampling occurs from the remaining tokens.
This can be done using various methods (e.g., temperature-adjusted sampling).

In the context of Large Language Models (LLMs) like Transformers, `tfs_z` is a hyperparameter that controls the **temperature scaling** of the output logits during text generation.

Here's what it does:

1. **Logits**: When an LLM generates text, it produces a probability distribution over all possible tokens in the vocabulary. This distribution is represented as a vector of logits (unnormalized log probabilities).
   
2. **Temperature scaling**: To control the level of uncertainty or "temperature" of the output, you can scale the logits by multiplying them with a temperature factor (`t`). This is known as temperature scaling.

3. **`tfs_z` hyperparameter**: It's a hyperparameter that controls how much to scale the logits before applying temperature scaling.

When you set `tfs_z > 0`, the model first normalizes the logits by subtracting their mean (`z-score normalization`) and then scales them with the temperature factor (`t`). This has two effects:

* **Reduced variance**: By normalizing the logits, you reduce the variance of the output distribution, which can help stabilize the generation process.
  
* **Increased uncertainty**: By scaling the normalized logits with a temperature factor, you increase the uncertainty of the output distribution, which can lead to more diverse and creative text generations.

In summary, `tfs_z` controls how much to scale the output logits after normalizing them. A higher value of `tfs_z` will produce more uncertain and potentially more creative text generations.

Keep in mind that this is a relatively advanced hyperparameter, and its optimal value may depend on the specific LLM architecture, dataset, and task at hand.

<br><br>
</details>

<details>
<summary>top_p (slider)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>min_p (slider)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>typical_p (slider)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>top_k (slider)</summary>
<br>

Teste

<br><br>
</details>

<br><br>
**Token Penalties:**


<details>
<summary>presence_penalty (slider)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>frequency_penalty (slider)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>repeat_penalty (slider)</summary>
<br>

Teste

<br><br>
</details>

<br><br>
**Others:**

<details>
<summary>Model metadata (textbox)</summary>
<br>

Teste

<br><br>
</details>


<details>
<summary>Show model's vocabulary (checkbox)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Unload model (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>PDF pages (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>PDF full (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>TXT system prompt (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>TXT user prompt (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Copy HF links (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>DB Browser (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>D-Tale (button)</summary>
<br>

Teste

<br><br>
</details>

<!--<details>
<summary>(button)</summary>
<br>

Teste

<br><br>
</details>-->

<br><br>
**Initial Settings:**

<details>
<summary>Settings</summary>
<br>

Samantha's initial settings is **deterministic**. As a rule, this means that for the same prompt, you'll get always the same answer, even when applying penalties to exclude repeated tokens (penalties does not affect the model deterministic behavior).<br> 

Used to assess training database biases. 

Some models tend to loop (repeat the same text indefinitely) when using highly deterministic adjustments, selecting tokens with the highest probability score. 

In turn, for **stochastic* behavior, suited for creative content, in which model selects tokens with different probability scores, adjust the hyperparameters accordingly.
<br><br>

üìê **Deterministic settings (default):**<br>
* temperature (0)
* tfs_z (0)
* top_p (0)
* min_p (1)
* typical_p (0)
* top_k (40)
* presence_penalty (0)
* frequency_penalty (0)
* repeat_penalty (1.1).
<br><br>

üé® **Stochastic settings:** <br>
* temperature (0.2)
* tfs_z (1)
* top_p (0.9)
* min_p (0.05)
* typical_p (1)
* top_k (40)
* presence_penalty (0)
* frequency_penalty (0)
* repeat_penalty (1.1)

<br><br>
</details>

<br><br>

### Interface Right Column (output):

<details>
<summary>Assistant output (textbox)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Next token (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Copy code blocks (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Open Jupyterlab (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Copy last response (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Copy all response (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Response in HTML (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Responses in HTML (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Voice control (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Audio player (widget)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Text to speech (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Last response (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>All responses (button)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Models repositories (links)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>Operating tips (tips)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>User prompt examples (prompts)</summary>
<br>

Teste

<br><br>
</details>

<details>
<summary>System prompt examples (prompts)</summary>
<br>

Teste

<br><br>
</details>

<br><br>
## ‚ñ∂Ô∏è Video Tutorials

<details>
<summary>Watch the videos</summary>

<br><br>
You can add text within a collapsed section. 
<br><br>
  
```python
   print("Hello World")
```
<br>

[![Watch the video](https://i.sstatic.net/Vp2cE.png)](https://youtu.be/vt5fpE0bzSY)

</details>

<br><br>
## ü™Ñ Samantha's Tips & Tricks

<details>
<summary>Tips & Tricks</summary>

<br><br>
* Keep only one interface window open at a time. Multiple open windows prevent the buttons from working properly.
* It is not possible to interrupt the program during the model loading and thinking phases, except by closing it. Pressing the stop buttons during these phases will only take effect when the token generation phase begins.
* You can select the same model more than once, in any sequence you prefer. To do so, select additional models at the bottom of the dropdown list. When deleting a model, all of the same type will be excluded from the selection.
* To select all text in a field, click inside the field and press CTRL + A.
* To create a new line in a field, press SHIFT + ENTER. If only the ENTER key is pressed, anywhere in the Samantha interface, the loading/processing/generation phases will begin.
* The pop-up window generated by the matplotlib module must be closed for the program to continue running.
* Whenever you save a new model on your computer, you must click on the "Load Model" button and select the folder where it is located so that it appears in the model selection dropdown.
* Changes made to the fields and interface settings during the model download and loading, processing and token generation will only be made the next time the Start Chat button or the ENTER key is pressed.
* To follow the text generation on the screen, click inside the _Assistant output_ field and press the Page Down key until you see the end of the text being generated by the model.
<br>

</details>


<br><br>
## üìù Version History and Future Improvements

<details>
<summary>Versions</summary>

 <br><br>
üóìÔ∏è **Code Versions:**

Version 0.1.0 (2024-08-05):
* Initial beta version.
<br>

üöÄ **Future improvements:**
<br>

* Try other lightweight offline Speech-to-Text libraries with better speech recognition accuracy.
* Refactor the code to make it more accessible.
<br>

Suggestions are always welcome!
<br>

</details>
<br><br>

<!--https://github.com/matiassingers/awesome-readme?tab=readme-ov-file#Examples-->
